{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8103eb54-f553-43b6-a468-1fb431d6b856",
   "metadata": {},
   "source": [
    "Данная работа является продолжением серии работ с общим названием ERNIE (см. [github-репозиторий](https://github.com/PaddlePaddle/ERNIE/blob/develop/README.en.md)):\n",
    "- [Sun et al., 2019]($ERNIE: Enhanced Representation through Knowledge Integration$). *ERNIE: Enhanced Representation through Knowledge Integration.*\n",
    "- [Sun et al., 2019]($ERNIE 2.0: A Continual Pre-training Framework for Language Understanding$). *ERNIE 2.0: A Continual Pre-training Framework for Language Understanding.*\n",
    "- [Xiao et al., 2020]($ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation$). *ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation.*\n",
    "- [Yu et al., 2020]($ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph$). *ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph.*\n",
    "- [Li et al., 2020]($UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning$). *UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning.*\n",
    "- [Ouyang et al., 2020]($ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora$). *ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora.*\n",
    "- [Xiao et al., 2020]($ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding$). *ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding.*\n",
    "- [Ding et al., 2020]($ERNIE-Doc: A Retrospective Long-Document Modeling Transformer$). *ERNIE-Doc: A Retrospective Long-Document Modeling Transformer.*\n",
    "- [Sun et al., 2021](https://arxiv.org/abs/2107.02137). *ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055fcc-8118-4a7b-b7dc-f712e8bd6531",
   "metadata": {},
   "source": [
    "<img src=\"assets/ERNIE_milestone_20210519_en.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6136c2b-3ff5-481f-915d-310c1d8b242a",
   "metadata": {},
   "source": [
    "### GPT и BERT: сравнение и объединение\n",
    "\n",
    "В модели ERNIE 3.0 авторы объединяют два подхода к построению и обучению языковых моделей:\n",
    "\n",
    "1. Auto-regressive, или left-to-right language model (GPT, [Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))\n",
    "2. Auto-encoding, или bidirectional language model (BERT, [Devlin et al., 2018](https://arxiv.org/abs/1810.04805))\n",
    "\n",
    "Авторы сравнивают плюсы и минусы GPT и BERT. По своей архитектуре BERT плохо подходит для генерации текстов (*natural language generation*): процесс генерации сложнее, а качество генерации обычно получается хуже, чем у GPT ([Wang and Cho, 2019]($BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model$)). Однако считается, что GPT хуже подходит для решения задач, связанных не пониманием текста (*natural language understanding*).\n",
    "\n",
    "> ...most large-scale models are trained in an auto-regressive way, but [Devlin et al., 2018](https://arxiv.org/abs/1810.04805) shows that such models demonstrate poorer performance with traditional fine-tuning when adapting to downstream language understanding tasks.\n",
    "\n",
    "Для того, чтобы модель ERNIE 3.0 могла качественно решать задачи как понимания, так и генерации текстов, авторы решили объединить оба подхода.\n",
    "\n",
    "> ERNIE 3.0 ... combines auto-regressive network and auto-encoding network so that the trained model can handle both natural language understanding and generation tasks through zero-shot learning, few-shot learning or fine-tuning.\n",
    "\n",
    "Гибридные языковые модели предлагались и ранее. Например, в XLNet ([Yang et al., 2019]($XLNet: Generalized Autoregressive Pretraining for Language Understanding$)) предлагается метод permutation language modeling, который реализуется с помощью модификации масок внимания; в UniLM ([Dong et al., 2019]($Unified Language Model Pre-training for Natural Language Understanding and Generation$)) предлагается обучать сеть параллельно в трех разных режимах: left-to-right, bidirectional и seq-to-seq, задавая соответствующие маски внимания.\n",
    "\n",
    "Архитектура ERNIE 3.0 состоит из трех блоков трансформера:\n",
    "- *Universal representation module* ($U$)\n",
    "- *Language understanding network* ($LU$)\n",
    "- *Language generation network* ($LG$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f05141-9cc0-4e4d-a792-f5bc352489f4",
   "metadata": {},
   "source": [
    "<img src=\"assets/ernie3.jpg\" width=\"700\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa9dc3-1a63-4cee-b5f8-eadbeaaa9db7",
   "metadata": {},
   "source": [
    "Каждый блок основан на [Transformer-XL]($Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context$) ([Dai et al., 2019](https://arxiv.org/abs/1901.02860)), обладающим механизмом памяти и способном учитывать большую длину контекста. Механизм рекурретности при этом модифицируется так, как предложено в ERNIE-Doc ([Ding et al., 2020]($ERNIE-Doc: A Retrospective Long-Document Modeling Transformer$)). Пусть некий текст разделен на несколько фрагментов. Отличие ERNIE-Doc от Transformer-XL состоит в следующем. На $n$-м слое $t$-го фрагмента текста в качестве keys и values используются не только векторы $(n-1)$-го слое $t$-го фрагмента, но и:\n",
    "\n",
    "- В Transformer-XL: векторы $(n-1)$-го слоя $(t-1)$-го фрагмента\n",
    "- В ERNIE-Doc: векторы $n$-го слоя $(t-1)$-го фрагмента\n",
    "\n",
    "При этом в ERNIE-Doc (и по-видимому в ERNIE 3.0) по всем фрагментам текста делается два прохода, первая фаза называется skimming, вторая - retrospective. Таким образом обеспечивается, что в фазе retrospective каждый фрагмент может использовать информацию обо всем тексте в целом (по крайней мере теоретически; на практике, как мне кажется, информация может экспоненциально забываться, особенно учитывая, что градиент не распространяется между фрагментами)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370ed2b-3036-42d9-b3ab-9733b14866f9",
   "metadata": {},
   "source": [
    "<img src=\"assets/erniedoc.jpg\" width=\"700\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35ef57-22ff-412f-9e42-5663f298f3dc",
   "metadata": {},
   "source": [
    "Если решается задача генерации текста, то модель ERNIE 3.0 работает как $LG(U(input))$, и используется треугольная маска внимания. Если же решается какая-либо другая задача, то модель работает как $LU(U(input))$, и маска внимания не используется.\n",
    "\n",
    "Авторы отмечают, что память в Transformer-XL может использоваться только при решении задач генерации текста (что довольно странно, поскольку технически ее можно использовать и в задаче masked language model).\n",
    "\n",
    "> And what needs special attention is that the memory module is only valid for natural language generation tasks while controlling the attention mask matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177248e-0ded-421d-b7ae-47bf0e7512c2",
   "metadata": {},
   "source": [
    "Обоснование выбора именно такой архитектуры для ERNIE 3.0, хоть и присутствует, но кажется недостаточным по двум причинам:\n",
    "\n",
    "1. Хотя GPT-подобные модели показывают меньшую точность при файн-тюнинге на задачах NLU, чем BERT-подобные модели но для них был предложен метод [P-tuning]($GPT Understands, Too$) ([Liu et al., 2021](https://arxiv.org/abs/2103.10385)), при использовании которого они, наоборот, опережают BERT-подобные модели.\n",
    "\n",
    "2. Авторы отказываются от seq-to-seq предобучения как от излишне сложного, хотя на моделях T5 ([Raffel et al., 2019]($Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer$)), T0 ([Sanh et al., 2021]($Multitask Prompted Training Enables Zero-Shot Task Generalization$)), OFA ([Wang et al., 2022]($Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework$)) и других было показано, что именно такой метод предобучения эффективен и более универсален.\n",
    "\n",
    "> Generative pre-training models usually utilize traditional language model (such as GPT, GPT-2) or sequence-to-sequence language model (such as BART, T5, ERNIE-GEN) as the pre-training task, the latter trains on the network with an auxiliary decoder structure. *ERNIE 3.0 opt for traditional language model as the pre-training task to abate the network complexity and heighten the effectiveness of unified pre-training.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c719f2-94f1-43ef-87d3-d2fc7e00df6e",
   "metadata": {},
   "source": [
    "### Использование графов знаний\n",
    "\n",
    "Языковые модели иногда обучают для того, чтобы работать с ними как с базами знаний ([Petroni et al., 2019]($Language Models as Knowledge Bases?$)). В ходе предобучения модель накапливает различные знания о мире, и иногда исследователи хотят повысить ее способность накапливать и использовать знания с помощью дополнительных методов:\n",
    "\n",
    "- [Sun et al., 2019]($ERNIE: Enhanced Representation through Knowledge Integration$) (ERNIE 1.0) модифицируют задачу masked language modeling, маскируя сущности в тексте целиком, вместо маскирования их случайных частей.\n",
    "- [Xiong et al., 2019]($Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model$) (WKLM) выполняют случайную замену сущностей в тексте и обучают модели распознавать такие замены.\n",
    "- [Wang et al., 2019]($KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation$) (KEPLER) параллельно с masked language modeling оптимизируют еще одну функцию потерь, для чего берутся две случайные сущности $A$, $B$ и связь $r$ между ними из графа знаний (например, $A$=\"Johannes Kepler\", $r$=\"Occupation\", $B$=\"astronomer\"). Обозначив за $e$ энкодер трансформера, за $d(A)$, $d(B)$ определение понятий $A$ и $B$ из энциклопедии, за $\\textit{emb}$ эмбеддинг, соответствующий типу связи $r$ в графе знаний и за $\\langle s \\rangle$ дополнительный токен, функция потерь специального вида рассчитывается на следующей тройке: $e(\\langle s \\rangle, d(A))$, $\\textit{emb}(r)$, $e(\\langle s \\rangle, d(B))$.\n",
    "- [Sun et al., 2020]($CoLAKE: Contextualized Language and Knowledge Embedding$) (CoLAKE) извлекают из текстов именованные сущности и ищут информацию о них в графе знаний, после чего составляют *word-knowledge graph*, подграфом которого является полносвязный граф слов в тексте. Далее энкодер трансформера работает с этим графом, при этом связность вершин в графе выражается в виде маски self-attention.\n",
    "- [Zhou et al., 2020]($Pre-training Text-to-Text Transformers for Concept-centric Common Sense$) (CALM) обучают seq-to-seq трансформер восстанавливать правильный порядок слов в предложении и отличать правильные предложения от предложений с замененными словами. Этот же трансформер генерирует для себя обучающие примеры, то есть работает как генератор и как дискриминатор.\n",
    "- [Zhang et al., 2022]($GreaseLM: Graph REASoning Enhanced Language Models for Question Answering$) (GreaseLM) обучают модель, которая принимает в качестве входных данных вопрос и участок графа знаний, содержащий те понятия, которые упоминаются в вопросе. Такой принцип работы похож на CoLAKE, но архитектура модели здесь сложнее: токены текста и элементы графа знаний обрабатываются по-разному."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4363f6c-ac64-451a-8e39-9f91a9f5c491",
   "metadata": {},
   "source": [
    "В обучении ERNIE 3.0 также используются графы знаний. Описанные выше модели имеют средний размер (12-24 слоев трансформера), тогда как авторы ERNIE 3.0 хотят протестировать использование графов знаний при обучении более крупной модели.\n",
    "\n",
    "ERNIE 3.0 обучается на нескольких задачах:\n",
    "\n",
    "**1. Knowledge masked language modeling.** Аналогично ERNIE 1.0, маскируются целиком отдельные сущности (Entity-level Masking) или фразы (Phrase-level Masking). В ERNIE 1.0 на первой стадии обучения использовалось обычное маскирование случайных токенов, на второй стадии - маскирование фраз, на третьей стадии - маскирование сущностей. Для поиска сущностей и фраз используются различные средства, специфичные для языка.\n",
    "\n",
    "> Phrase is a small group of words or characters together acting as a conceptual unit. For English, we use lexical analysis and chunking tools to get the boundary of phrases in the sentences ([Sun et al., 2019]($ERNIE: Enhanced Representation through Knowledge Integration$))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85601e6-b3ca-4e56-b9b9-1f58872dfaf6",
   "metadata": {},
   "source": [
    "<img src=\"assets/knowledgemasking.jpg\" width=\"700\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed5c17-826f-4bd7-9047-7ee210557df9",
   "metadata": {},
   "source": [
    "**2. Document language modeling.** Задача предсказания следующего слова в тексте (generative pre-training), при которой по очереди обрабатываются все фрагменты документа с использованием рекуррентности по аналогии с ERNIE-Doc.\n",
    "\n",
    "**3. Sentence reordering.** Входными данными являются $k$ сегментов текста, перемешанных случайным образом, и задачей модели является определение правильного порядка. Рассматривается как задача классификации на $k!$ классов, где каждый класс соответствует определенному порядку сегментов.\n",
    "\n",
    "**4. Sentence distance.** Входными данными являются 2 сегмента текста, и задачей модели является определить, следуют ли сегменты друг за другом в исходном тексте, или если нет, то взяты ли они из одного и того же текста. Рассматривается как задача классификации на 3 класса.\n",
    "\n",
    "**5. Universal knowledge-text prediction.** Входными данными является связь из графа знаний, связывающая два понятия, и предложение из энциклопедии, содержащее эти же два понятия (как показано на изображении). Маскируется либо связь между понятиями, либо случайное слово из предложения. Задачей модели является определить слова, закрытые маской."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf069b-e1a0-46b1-82fa-690afcab9bb8",
   "metadata": {},
   "source": [
    "<img src=\"assets/knowledgemasking2.jpg\" width=\"700\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd22a0-93c2-461d-9635-e1b13a899702",
   "metadata": {},
   "source": [
    "> To incorporate knowledge into one pre-trained language model, we introduce universal knowledge-text prediction task ... To predict the relation in the triple, the model needs to detect mentions of head entity and tail entity and determine semantic relationship that holds between them in the corresponding sentence.\n",
    "\n",
    "Из перечисленных задач только задача Document language modeling выполняется авторегрессивно.\n",
    "\n",
    "> ERNIE 3.0 trains the **NLU network** through **knowledge masked language modeling** to improve the capacity of capturing the lexical information, trains the **sentence reordering task** and the **sentence distance discerning** task to strengthen the ability of capturing the syntactic information, and finally optimizes the model with the **universal knowledge-text prediction task** to improve knowledge memorization and reasoning. Meanwhile, ERNIE 3.0 trains the **NLG network** with the **document language modeling task** to enable various generation styles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17413c26-8b81-4928-824e-697bc64ae0ca",
   "metadata": {},
   "source": [
    "### Данные для обучения\n",
    "\n",
    "Авторы собрали самый большой среди существующих аналогов корпус китайских текстов размером 4 Тб (раздел 3.3.2). Удаление дубликатов было выполнено на нескольких уровнях. Во-первых удалены дублирования символов: пробелов, табуляции, вопросительных и восклицательных знаков и пр. Во-вторых удалены дублирования параграфов, если в каждом параграфе от 1 до 99 предложений. Согласно авторам, эти два шага предобработки очень важны для того, чтобы ERNIE 3.0 не генерировала тексты с повторениями. Кроме того, авторы удалили дублирования документов с помощью сравнения суммы хешей MD5 трех самых длинных предложений в каждом документе.\n",
    "\n",
    "Предложения размером меньше 10 слов были отфильтрованы, поскольку среди них могут быть поврежденные или неполные. Этот пункт остается не совсем понятным: во-первых во многих языках большая часть предложений содержат меньше 10 слов, во-вторых удаление отдельных предложений из параграфов нарушает связность текста.\n",
    "\n",
    "В целом многие аспекты предобработки и фильтрации корпуса текстов специфичны для китайского языка.\n",
    "\n",
    "> We further conduct sentence segmentation using regular expressions and word segmentation based on Baidu’s\n",
    "word segmentation tool. This helps ERNIE 3.0 to learn better sentence boundary and named entity knowledge\n",
    "during pre-training.\n",
    "\n",
    "> Then, each dataset is multiplied by a user-defined multiplier number to increase the data diversity after truncating the data for NLU-network pre-training.\n",
    "\n",
    "Также авторы использовали граф знаний Baidu, содержащий более 50 миллионов фактов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61ea60-b3aa-4047-a91f-7bc24bd61d74",
   "metadata": {},
   "source": [
    "### Детали архитектуры\n",
    "\n",
    "*Universal representation module* имеет 48 слоев, 64 головы и размер вектора 4096. *Language understanding network* и *Language generation network* имеют по 12 слоев, 12 голов и размер вектора 768. В сумме сеть имеет около 10 миллиардов параметров.\n",
    "\n",
    "Используется функция активации [GELU](https://paperswithcode.com/method/gelu) (Gaussian Error Linear Unit), которая использовалась в BERT и GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f536e2f-d121-47a2-b5af-76157b4a1d8f",
   "metadata": {},
   "source": [
    "### Процесс обучения\n",
    "\n",
    "Авторы использовали следующие гиперпараметры (см. разел 3.3.3):\n",
    "\n",
    "> The maximum sequence length of context and the memory length of language generation is set to 512 and 128, respectively. The total batch size of all pre-training tasks is set to 6144. We use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps and linear decay of the learning rate. In the first 10,000 steps, we also use the progressive learning to speedup convergence in the initial stage of pre-training. The model is trained for a total of 375 billion tokens with 384 NVDIA v100 GPU cards and is implemented on [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) framework. By virtue of parameter sharding used in [[Rajbhandari et al., 2019]($ZeRO: Memory Optimizations Toward Training Trillion Parameter Models$); [Ramesh et al., 2021]($Zero-Shot Text-to-Image Generation$)], we manage to reduce the memory usage of our model and address the problem of the total parameter of model exceeding the memory of a single GPU card.\n",
    "\n",
    "В процессе обучения изменяются следующие параметры (см. разел 3.3.1):\n",
    "\n",
    "1. В процессе обучения увеличивается длина входной последовательности токенов. Авторы ссылаются на BERT ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)), где первые 90% шагов модель обучалась на уменьшенной длине последовательности.\n",
    "2. В процессе обучения увеличивается размер батча. Авторы ссылаются на GPT-3 ([Brown et al., 2020]($Language Models are Few-Shot Learners$), см. Appendix B), где размер батча в процессе обучения увеличивался с 32 тысяч токенов (16 примеров по 2048 токенов) до 4-12 миллионов токенов (2-8 тысяч примеров по 2048 токенов), в зависимости от размера модели. Увеличивать размер батча в ходе обучения предлагается также в [Smith et al., 2017]($Dont Decay the Learning Rate, Increase the Batch Size$).\n",
    "3. Увеличивается learning rate и dropout. Увеличение learning rate хорошо соотносится с существующей практикой learning rate warmup при обучении трансформеров ([Popel and Bojar, 2018]($Training Tips for the Transformer Model$)). Авторы ссылаются на EfficientNetV2 ([Tan and Le, 2021]($EfficientNetV2: Smaller Models and Faster Training$)), где постепенно увеличивается размер изображения и одновременно параметры регуляризации: dropout, mixup ratio и RandAugment magnitude.\n",
    "> ...when training with different image sizes, we should also adjust the regularization strength accordingly ... In fact, it is common that large models require stronger regularization to combat overfitting: for example, EfficientNet-B7 uses larger dropout and stronger data augmentation than the B0. In this paper, we argue that even for the same network, smaller *image size* leads to smaller network capacity and thus needs weaker regularization; vice versa, larger image size leads to more computations with larger capacity, and thus more vulnerable to overfitting. ([Tan and Le, 2021]($EfficientNetV2: Smaller Models and Faster Training$))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfa75e-a9ee-4046-ac52-181341b278db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
