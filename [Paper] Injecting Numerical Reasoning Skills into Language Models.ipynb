{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e4d592-74a7-4aff-b268-c3b614dcad6d",
   "metadata": {},
   "source": [
    "В данной работе описана модель GenBERT для решения NLP-задач, которые требуют арифметических вычислений.\n",
    "\n",
    "В последнее время получили распространение языковые модели - нейронные сети, обученные на задачах языкового моделирования. Например, модель GPT обучается на задаче, в которой нужно предсказывать продолжение текста, а модель BERT обучается сразу на нескольких задачах: определять слова, закрытые маской, исправлять слова, замененные случайным образом, а также определять, является ли один текст прямым продолжением другого. В результате такие модели научаются хорошо работать с текстами и могут быть дообучены для решения различных практических задач. Более подробно я писал о GPT и BERT в [одном из предыдущих обзоров]($GPT и BERT$).\n",
    "\n",
    "Однако модели, обученные на задаче языкового моделирования, испытывают трудности при решении арифметических задач (numerical reasoning over text, NRoT). Например, датасет [DROP](https://paperswithcode.com/dataset/drop) состоит из текстов, вопросов и ответов по ним. К примеру, тексту по теме американского футбола соответствуют такие вопросы:\n",
    "\n",
    "- \"How many total yards did Phil Dawson throw for touchdowns?\"\n",
    "- \"Who threw 45 total yards for touchdowns?\"\n",
    "\n",
    "В первом случае ответом является результат вычислений, а во втором случае ответом является имя, для его получения которого нужно выполнить вычисления. Модель BERT сама по себе дает не очень хорошие результаты при файн-тюнинге на датасете DROP. В данной работе авторы решают эту проблему с помощью нового способа токенизации чисел и предобучения на синтетическом датасете, состоящем из арифметических задач."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d67b5c-cc20-4bc9-8fce-44216ddd30f1",
   "metadata": {},
   "source": [
    "### Токенизация цифр (digit tokenization)\n",
    "\n",
    "Способ токенизации [WordPiece]($Googles Neural Machine Translation System: Bridging the Gap between Human and Machine Translation$), используемый в BERT, работает с числами так же, как со словами. Авторы текущей работы предлагают другой способ, в котором токенами являются отдельные цифры в числе. Это означает, что для каждой цифры вводится эмбеддинг, и число 123 будет преобразовано в последовательность из трех векторов: эмбеддинг числа 1, эмбеддинг числа 2 и эмбеддинг числа 3.\n",
    "\n",
    "Конечно можно придумать и другие способы создания эмбеддингов чисел. Например, можно превращать число в один вектор-эмбеддинг, складывая эмбеддинг нуля и эмбеддинг единицы, умноженный на данное число. Но предложенный авторами способ хорош во-первых тем, что даже при работе с очень большими числами нормы эмбеддингов никогда не будут стремиться к бесконечности, а во-вторых тем, что иногда числа могут быть представлены в виде слов (\"fifty nine and a half\"), и способ кодирования чисел, записанных словами или цифрами, получается согласованным."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703a93-a653-4b92-a9b7-8e1f9c7f9657",
   "metadata": {},
   "source": [
    "### Синтетические датасеты для предобучения\n",
    "\n",
    "Авторами разработаны два синтетических (автоматически генерируемых) датасета, которые используются для предобучения модели GenBERT. Датасет **Numerical Data (ND)** состоит только из операций над числами и датами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fd025-693b-4df8-88e7-cd0603bbafcb",
   "metadata": {},
   "source": [
    "<img src=\"assets/genbert.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d081302-a4dd-4a51-b33a-cca253ad2a31",
   "metadata": {},
   "source": [
    "При обучении на датасете Numerical Data осуществлялся случайный сдвиг входных данных на величину до 512 токенов. Или, иначе говоря, позиционные эмбеддинги (см. [Attention Is All You Need]($Attention Is All You Need$), раздел \"Позиционное кодирование\") сдвигались в обратную сторону. Это делалось для того, чтобы модель умела работать с числами, находящимися на любой позиции в тексте.\n",
    "\n",
    "Еще один датасет **Textual Data (TD)** содержит контексты и вопросы, для получения ответов на которые требуется выполнять арифметические операции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b1e5a-36fb-48fa-8215-d761096cc6f7",
   "metadata": {},
   "source": [
    "<img src=\"assets/genbert2.jpg\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c4a05-9d97-4418-9a18-1d94b014110f",
   "metadata": {},
   "source": [
    "Со своей стороны отмечу возможный недостаток такого подхода: модель при предобучении накапливает информацию, взятую из текстов, и предобучение на таком датасете может повлиять на то, верные ли исторические факты запомнит модель. С другой стороны, высокая вариативность чисел и слов в синтетическом датасете может сгладить эту проблему."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1796d9fc-4270-4623-8480-f0aff04cdfec",
   "metadata": {},
   "source": [
    "### Архитектура и обучение модели GenBERT\n",
    "\n",
    "Синтетические датасеты построены либо как пары (вопрос + ответ), либо как тройки (контекст + вопрос + ответ). В некоторых случаях ответ может быть найден в вопросе, то есть достаточно указать промежуток (span) в вопросе, который и является ответом. В этом случае для каждого токена модель выдает вероятности того, что он является началом или концом span'а. Более подробно о формате входных данных, формате выходных данных и функции потерь в такой постановке задачи я писал в [обзоре на BERT]($GPT и BERT$), раздел \"Дообучение на SQuAD\". В данном случае ситуация аналогичная.\n",
    "\n",
    "В некоторых случаях ответ может не содержаться в вопросе, например если нужно посчитать сумму чисел. В этом случае ответ нужно сгенерировать, и для этого модель дополняется декодером трансформера (подробнее об энкодере и декодере я писал в [этом обзоре]($Attention Is All You Need$); для понимания принципа работы энкодера и декодера достаточно прочитать раздел \"Общее устройство трансформера\"). Декодер в GenBERT использует те же веса, что и энкодер. Self-attention блок в декодере, направленный на энкодер, использует те же веса, что и другой self-attention блок, направленный на сам декодер. Чтобы энкодер и декодер могли работать по-разному, на выходе энкодера и декодера добавляется еще один полносвязный слой с функцией активации GeLU и операцией LayerNorm. Этот слой инициалирзируется разными весами в энкодере и декодере (на схеме слой обозначен как **FFenc** и **FFdec**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed057e-04f7-4a5e-92a2-c7db692d3ce8",
   "metadata": {},
   "source": [
    "<img src=\"assets/genbert3.jpg\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26f6c2-3389-4808-8a8a-b2e7b1c66bfb",
   "metadata": {},
   "source": [
    "Модель GenBERT при обучении на синтетическом датасете использует четыре головы:\n",
    "\n",
    "1. Голова **question span** присоединена к выходу энкодера и ищет ответ в вопросе, то есть выдает распределение вероятностей начала и конца span'а по выходным токенам, соответствующим вопросу (подробнее см. в [обзоре на BERT]($GPT и BERT$), раздел \"Дообучение на SQuAD\"). На схеме вопросу соответствуют входные токены $q_i$.\n",
    "2. Голова **context span** присоединена к выходу энкодера и таким же образом ищет ответ в контексте, то есть в тексте, по которому задан вопрос.  На схеме контексту соответствуют входные токены $p_i$.\n",
    "3. **Выход декодера** является третьей головой.\n",
    "4. Голова **type** присоединена к выходу энкодера и выдает распределение вероятностей по остальным трем головам, то есть отвечает на вопрос о том, с какой головы нужно считывать ответ.\n",
    "\n",
    "Функция потерь в декодере является стандартной (см. [обзор на трансформер]($Attention Is All You Need$), раздел \"Принцип обучения трансформера\"). Формально условная вероятность отета $\\langle a \\rangle$, то есть последовательности токенов $[SOS], a_1, \\dots, a_m, [EOS]$, при контексте $\\textbf{c}$ и вопросе $\\textbf{q}$ вычисляется с помощью декодера следующим образом:\n",
    "\n",
    "$p_\\text{dec}(\\langle a \\rangle\\ |\\ \\textbf{c}, \\textbf{q}) = \\prod\\limits_{i=0}^m p_\\text{dec}(a_{i+1} | a_0, \\dots, a_i, \\textbf{c}, \\textbf{q})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf1252-c672-431b-a49d-a862f5fdee0e",
   "metadata": {},
   "source": [
    "Каждая голова выдает распределение вероятностей по возможным ответам, и умножая это распределение на вероятность данной головы (выданную головой **type**), мы получаем финальные вероятности. В качестве функции потерь используется кроссэнтропия (logloss), то есть минимизация минус логарифма вероятности верного ответа.\n",
    "\n",
    "Формально общая функция потерь вычисляется таким образом:\n",
    "\n",
    "$-\\log \\Big( \\textbf{p}_\\textbf{q} p_\\text{dec}(\\langle a \\rangle\\ |\\ \\textbf{c}, \\textbf{q}) + \\textbf{p}_\\textbf{q} \\sum\\limits_{(i, j) \\in S} p_q (i, j\\ |\\ \\textbf{c}, \\textbf{q}) + \\textbf{p}_\\textbf{c} \\sum\\limits_{(i, j) \\in S} p_c (i, j\\ |\\ \\textbf{c}, \\textbf{q}) \\Big)$\n",
    "\n",
    "Фактически в данной формуле вероятность считается как взвешенное среднее по трем головам, где веса - вероятности голов. Вектор [$\\textbf{p}_\\textbf{q}$, $\\textbf{p}_\\textbf{c}$, $\\textbf{p}_\\textbf{dec}$] - это эталонное распределение вероятностей по трем головам, взятым из разметки (вероятно это единица для верной головы и ноль для неверных голов).\n",
    "\n",
    "Важно также, что в процессе обучения на синтетическом датасете модель GenBERT параллельно *продолжает обучаться на задаче языкового моделирования* (на которой обучалась BERT), чтобы модель не \"забыла\", как работать с текстом, не содержащим числа. На каждом шаге обучения батч обучающих данных является конкатенацией трех батчей:\n",
    "\n",
    "1. Батч из синтетического датасета Numerical Data (ND)\n",
    "2. Батч из синтетического датасета Textual Data (TD)\n",
    "3. Батч из задачи Masked Language Model (MLM) (подробнее см. [GPT и BERT]($GPT и BERT$)).\n",
    "\n",
    "Суммарная функция потерь при этом является взвешенной суммой трех функций потерь по этим трем типам задач."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac7b67-7a89-4791-be4c-bbd4ab7e71eb",
   "metadata": {},
   "source": [
    "**Достигнутые результаты**\n",
    "\n",
    "Авторы сравнивают две модели:\n",
    "1. Оригинальную модель BERT\n",
    "2. Модель BERT, которая дообучалась на синтетическом датасете с использованием токенизацией цифр\n",
    "\n",
    "Вторая модель показывала намного более высокое качество при файн-тюнинге на датасете DROP (о котором упоминалось выше). Это происходит без потери качества файн-тюнинга на других задачах. Чтобы проверить это, авторы файн-тюнили обе модели на датасете SQuAD, где не требуется работа с числами, и получали примерно одинаковые результаты.\n",
    "\n",
    "Преимущество модели GenBERT в том, что она достаточно проста и универсальна, при этом умея явно или неявно работать с числами и датами: складывать, вычитать, искать средние, максимальные и минимальные значения. Тем не менее, модель не обучена для решения некоторых других классов задач, таких как сортировка чисел, а также плохо работает с арифметическими выражениями, где встречается более трех чисел (см. разделы 5.1, 5.2 статьи).\n",
    "\n",
    "Авторы предполагают, что эти ограничения могут быть преодолены путем добавления в синтетический датасет новых типов примеров. Хотя, с моей точки зрения, остается неясным: не будет ли необходимый объем предобучения расти экспоненциально в зависимости от длины арифметических выражений? Если будет, то для работы с длинными арифметическими выражениями нужен иной подход."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
