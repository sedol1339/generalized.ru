{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c370769-779b-4e5e-9832-22d00ae98387",
   "metadata": {},
   "source": [
    "(Введение, о содержании статьи, ссылки на работы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753baab-3818-4323-928b-072ef152c859",
   "metadata": {},
   "source": [
    "## Интерпретация моделей машинного обучения\n",
    "\n",
    "Модели машинного обучения (такие как нейронные сети, машины опорных векторов, ансамбли решающих деревьев) являются \"прозрачными\" в том смысле, что все происходящие внутри них вычисления известны. Но тем не менее часто говорят, что модели машинного обучения плохо интерпретируемы. Здесь имеется в виду то, что процесс принятия решения не удается представить в понятной человеку форме, то есть:\n",
    "\n",
    "1. Понять, какие признаки или свойства входных данных влияют на ответ\n",
    "2. Разложить алгоритм принятия решения на понятные составные части\n",
    "3. Объяснить смысл промежуточных результатов, если они есть\n",
    "3. Описать в текстовом виде алгоритм принятия решения (возможно, с привлечением схем или графиков)\n",
    "\n",
    "Достичь полной интерпретируемости в машинном обучении, как правило, не удается, но даже частичная интерпретация может существенно помочь. Если мы узнаем, на что именно обращает внимание алгоритм, какими правилами руководствуется, то сможем оценить правдоподобность этих правил и тем самым распознать переобучение. Также мы сможем лучше понять, почему модель ошибается на тех или иных примерах, что может помочь улучшить алгоритм обучения.\n",
    "\n",
    "> Humans usually have prior knowledge about the application domain, which they can use to accept (trust) or reject a prediction if they understand the reasoning behind it. ([Ribeiro et al., 2016]($Why Should I Trust You?: Explaining the Predictions of Any Classifier$))\n",
    "\n",
    "Кроме того, интерпретировав алгоритм, мы можем открыть для себя что-то новое о свойствах исследуемых данных (например, какие признаки в табличных данных в наибольшей степени влияют на ответ).\n",
    "\n",
    "> The need to explain predictions from tree models is widespread. It is particularly important in medical applications, where the patterns uncovered by a model are often even more important than the model’s prediction performance. ([Lundberg et al., 2019]($Explainable AI for Trees: From Local Explanations to Global Understanding$))\n",
    "\n",
    "Обзор способов интерпретации моделей машинного обучения можно найти, например, в [Linardatos et al., 2020]($Explainable AI: A Review of Machine Learning Interpretability Methods$) и [Li et al., 2021]($Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08170bd4-0fde-4309-871c-c95d47d79a1b",
   "metadata": {},
   "source": [
    "### Интерпретация моделей для оценки качества\n",
    "\n",
    "Авторы методa LIME ([Ribeiro et al., 2016]($Why Should I Trust You?: Explaining the Predictions of Any Classifier$)) делают акцент на ситуации, когда требуется оценить качество обученной модели (например, чтобы сделать выбор между одной из нескольких моделей). Оценка точности на тестовой выборке не всегда позволяет хорошо оценить качество из-за следующих проблем:\n",
    "\n",
    "1. <p><b>Сдвиг данных</b> (data shift). Эта ситуация означает, что данные, на которых модель будет применяться, среднестатистически отличаются от тех, на которых модель обучалась и тестировалась, то есть распределение входных данных отличается при тестировании и применении: $P_{test}(X, Y) \\neq P_{usage}(X, Y)$. Поскольку $P(X, Y) = P(Y|X)P(X)$, то двумя вариантами сдвига данных является сдвиг в исходных данных $P(X)$ и сдвиг целевой переменной $P(Y|X)$.</p>\n",
    "<p>Одна из проблем сдвига данных в том, что большинство метрик качества (accuracy, MSE, logloss, F1 и другие) зависят от распределения исходных данных $P(X)$, то есть, упрощенно говоря, от соотношения разных типов примеров в датасете. Например, пусть модель тестировалась на датасете, в котором 80% изображений были высокого качества (HQ), а применяться будет в условиях, когда, наоборот, 80% изображений будут низкого качества (LQ). Пусть мы сравниваем две модели: на HQ-изображениях точность первой модели лучше, чем второй, а на LQ-изображениях, наоборот, точность второй модели лучше, чем первой. Если при тестировании большая часть изображений были HQ, то мы сделаем вывод, что первая модель лучше, тогда как на самом деле лучше была бы вторая.</p>  \n",
    "<p>Другая проблема сдвига данных в том, что в обучающих и тестовых данных могут присутствовать такие корреляции, которые не обобщаются на другие выборки. Например, мы классифицируем животных по изображению, но большинство изображений рыб, которые у нас имеются, содержат также пальцы рыбака (эти изображения мы используем как при обучении, так и при тестировании). Модель может научиться классифицировать как рыбу изображение, содержащее пальцы, что в целом неверно и может не работать на других выборках (см. также <a href=$Inductive Biases for Deep Learning of Higher-Level Cognition$>Goyal and Bengio, 2020</a> и обзор <a href=$Априорные гипотезы и регуляризация в машинном обучении$>Априорные гипотезы и регуляризация в машинном обучении</a>; пример с рыбами см. в <a href=$Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet$>Brendel and Bethge, 2019</a>).</p>\n",
    "\n",
    "2. **Утечка данных** (data leakage, или target leakage). Например, ID пациента может сильно коррелировать с диагнозом, но только в текущем датасете (который поделен на обучающую и тестовую часть). Модель, предсказывающая диагноз по ID, будет иметь высокую точность на тестовом датасете, но в целом очевидно, что в данной задаче такой способ предсказания некорректен и не будет хорошо работать на других данных. Утечка данных является частным случаем сдвига данных, поскольку зависимость ID $\\to$ диагноз была в $P_{train}(X, Y)$ и $P_{test}(X, Y)$, но ее не будет в $P_{usage}(X, Y)$. Избавиться от утечки данных не всегда просто.\n",
    "\n",
    "> Еxample for this is KDD-Cup 2008 breast cancer prediction competition, where the patient ID contained an obvious leak. It is by no means obvious that removing this feature would leave a leakage-free dataset, however. Assuming different ID ranges correspond to different health care facilities (in different geographical locations, with different equipment), there may be additional traces of this in the data. If for instance the imaging equipment's grey scale is slightly different and in particular grey levels are higher in the location with high cancer rate, the model without ID could pick up this leaking signal from the remaining data, and the performance estimate\n",
    "would still be optimistic (the winners show evidence of this in their report). ([Kaufman et al., 2011]($Leakage in Data Mining: Formulation, Detection, and Avoidance$))\n",
    "\n",
    "Если удастся интерпретировать модель (хотя бы приблизительно), то мы получим дополнительную информацию, которая поможет надежнее оценить ее качество в условиях возможной утечки и сдвига данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0853a80-ba28-4d45-8046-dccf84c030f4",
   "metadata": {},
   "source": [
    "### Локальная интерпретация моделей\n",
    "\n",
    "Вместо попыток интерпретировать модель целиком, что может быть очень сложно, мы можем рассмотреть задачу интерпретации ответа модели $f$ на конкретном, фиксированном примере $x_0$. Например, если на данном изображении модель распознала собаку, то почему она распознала собаку? Какие части и свойства изображения повлияли на предсказание модели?\n",
    "\n",
    "Для ответа на этот вопрос мы можем изменять $x_0$ и смотреть, как изменится при этом ответ модели, то есть мы изучаем зависимость $f(x_0 + \\Delta x)$ от $\\Delta x$. При этом возможно удастся с хорошей точностью аппроксимировать эту зависимость простой функцией $g(\\Delta x)$. Такой подход называется локальной аппроксимацией модели в окрестности точки $x_0$.\n",
    "\n",
    "Например, рассчитав градиент $\\nabla f(x)$ в точке $x_0$ мы узнаем, как изменится ответ при очень малых изменениях $\\Delta x$. При этом мы получаем локальную линейную аппроксимацию (в курсе высшей математики такая аппроксимация называется дифференциалом функции $f$). Такой подход используется, например, при расчете так называемых saliency maps в компьютерном зрении ([Simonyan et al., 2013]($Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps$)) - производных выходных значений сети по отдельным пикселям изображения. Но такая аппроксимация далеко не всегда адекватна:\n",
    "\n",
    "1. Производная локальна и не говорит о том, как изменится ответ при существенных изменениях $\\Delta x$. Например, если один из признаков достиг состояния \"насыщения\", то есть значение данного признака более чем достаточно, чтобы сделать какой-то вывод о целевой переменной, то производная по нему почти равна нулю. Эффект будет лишь если мы сильно изменим данный признак.\n",
    "2. В некоторых моделях (решающих деревьях) производная либо равна нулю, либо не существует.\n",
    "3. Для бинарных признаков производная не всегда информативна, поскольку малое изменение признака ведет в \"невозможную\" область нецелого значения, в котором модель и не обязана работать корректно.\n",
    "\n",
    "Есть и другой подход к локальной интерпретации модели: в качестве $\\Delta x$ мы можем рассматривать некий набор осмысленных, не бесконечно малых изменений входных данных. Именно такой подход лежит в основе метода интерпретации LIME, который мы рассмотрим далее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab462f-1bb5-40e4-9597-38154658adff",
   "metadata": {},
   "source": [
    "## LIME: Local Interpretable Model-agnostic Explanations\n",
    "\n",
    "LIME - это подход к интерпретации ответа модели $f(x_0)$ на конкретном тестовом примере $x_0$ с помощью вычисления значений $f(x_0 + \\Delta x)$ для некоторого конечного набора значений $\\Delta x$. Иллюстрация работы метода LIME приведена на *рис. 1*. Чтобы формально описать метод LIME, нам понадобится ввести ряд обозначений, которые мы будем использовать и в следующих частях обзора."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f102b-1164-44b2-a330-25cdc78dae4d",
   "metadata": {},
   "source": [
    "<img src=\"assets/lime2.jpg\" width=\"600\" align=\"center\">\n",
    "\n",
    "<center><i>Рис. 1. Иллюстрация работы метода LIME.</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de3f38-1d05-42bd-b8a1-8d3cfd321831",
   "metadata": {},
   "source": [
    "Мы интерпретируем модель $f$ на примере $x_0$:\n",
    "\n",
    "- $f: X \\to Y$ - исходная модель\n",
    "- $x_0 \\in X$ - выбранный тестовый пример, предсказание на котором $f(x_0)$ интерпретируется\n",
    "\n",
    "### Локальное упрощенное представление\n",
    "\n",
    "Для примера $x_0$ вводим $M$ осмысленных, интерпретируемых изменений $\\Delta_i$. Например, для изображений таким изменением может быть удаление отдельного суперпикселя, то есть участка изображения с похожим содержимым и четкими границами (*рис. 1b*). Каждое изменение бинарно, то есть оно либо есть, либо нет. Соответственно, мы получаем $2^M$ различных вариантов $\\Delta x$. Наличие или отсутствие каждого из изменений можно описать числом 0 или 1: из этих чисел можно собрать бинарный вектор $z$ размерностью $M$, который будем называть *упрощенным представлением* (*рис. 1c*).\n",
    "\n",
    "- $\\{ \\Delta_i \\}_{i=1}^M$ - изменения примера $x_0$\n",
    "- $z_i \\in \\{0, 1\\}$ - индикатор изменения $\\Delta_i$\n",
    "- $z \\in \\{0, 1\\}^M$ - вектор упрощенного представления\n",
    "- $h: \\{0, 1\\}^M \\to X$ - функция, преобразующая вектор упрощенного представления $z$ в $x_0 + \\Delta x$\n",
    "\n",
    "Например, на *рис. 1* функция $h$ работает следующим образом: все \"пристствующие\" суперпиксели ($z_i = 1$) рисуются без изменений. Все \"отсутствующие\" суперпиксели ($z_i = 0$) заполняются белым цветом (либо, как вариант, усредненным цветом соседних суперпикселей). При этом $h([1, 1, \\dots, 1]) = x_0$, поскольку вектор из единиц означает отсутствие всех изменений. Фактически выбор функции $h$ равносилен выбору изменений $\\Delta_i$, то есть семантики упрощенного представления.\n",
    "\n",
    "Функцию $h$ мы можем выбрать произвольно, но так, чтобы отдельные изменения $\\Delta_i$ были интерпретируемы. При этом мы надеемся, что объяснить предсказание модели на $x_0$ можно интерпретировать, изучая, как влияют на ответ эти изменения. Конечно, функция $g$ может быть выбрана неудачно. Например, если модель определяет стиль фотографии как \"ретро\" при наличии оттенка \"сепия\", то изменения отдельных суперпикселей не помогут интерпретировать модель. Если $\\Delta_i$ были выбраны неудачно, то всегда можно попробовать заново с другими изменениями $\\Delta_i$.\n",
    "\n",
    "Для табличных данных мы можем в качестве $\\Delta_i$ рассматривать замену одного из признаков на ноль, среднее или медианное значение по обучающему датасету."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726586b-eb93-4821-be0e-58a021543f55",
   "metadata": {},
   "source": [
    "### Объясняющая модель\n",
    "\n",
    "Теперь мы можем обучить модель $g$ предсказывать значение $f(h(z))$ по вектору упрощенного представления $z$.  При этом модель $g$ должна быть простой и интерпретируемой (поскольку смысл метода LIME в интерпретации). Например, это может быть линейная модель или решающее дерево. Чтобы обучить модель, нужно собрать обучающую выборку. Для этого нам потребуется получить ответ модели $f$ для разных $z$ (то есть для разных $x_0 + \\Delta x$, поскольку $z$ определяет $\\Delta x$) и таким образом собрать обучающую выборку для модели $g$.\n",
    "\n",
    "- $g: \\{0, 1\\}^M \\to Y$ - объясняющая модель\n",
    "- $\\big\\{ z^{(i)}, f(h(z^{(i)})) \\big\\}_{i=1}^N$ - обучающая выборка для объясняющей модели\n",
    "\n",
    "Максимально возможный размер обучающей выборки равен $2^M$, но обычно $M$ велико, и приходится ограничиться перебором лишь некоторых значений $z$. Авторы предлагают уделять основное внимание таким $z$, которые близки к вектору из единиц: это соответствует небольшим изменениям в $x_0$ (чем больше нулей в $z$, тем больше одновременных изменений $x_0$ мы рассматриваем). Введем функцию $\\pi(z)$, определяющую меру близости $h(z)$ к $x_0$, и назначим веса $\\pi(z^{(i)})$ примерам из обучающей выборки.\n",
    "\n",
    "- $\\pi: \\{0, 1\\}^M \\to \\mathbb{R}$ - мера близости $h(z)$ к $x_0$\n",
    "- $\\big\\{ w^{(i)} = \\pi(z^{(i)}) \\big\\}_{i=1}^N$ - веса примеров из обучающей выборки\n",
    "\n",
    "*Примечание.* В kernel SHAP, который мы рассмотрим в следующих разделах, в качестве $\\pi$ берется функция, назначающая большие веса как векторам с большим количеством единиц, так и векторам с большим количеством нулей. Так мы акцентируем внимание в том числе на значениях $z$ с большим количеством нулей, то есть на отдельных интерпретируемых компонентах (напрмер, отдельных суперпикселях в случае изображений).\n",
    "\n",
    "Для обучения модели $g$ осталось выбрать функцию потерь - например, среднеквадратичное отклонение. Эта функция будет сравнивать предсказания моделей $f$ и $g$. Авторы предлагают также использовать \"штраф за сложность\" $\\Omega(g)$ - например, количество ненулевых весов в линейной модели. Если в качестве функции потерь выбрано среднеквадратичное отклонение, то задача оптимизации формулируется следующим образом:\n",
    "\n",
    "$\\sum\\limits_{i=1}^N w^{(i)} \\Big( g(z^{(i)}) - f(h(z^{(i)})) \\Big)^2 + \\Omega(g) \\to \\min\\limits_g$\n",
    "\n",
    "В данной формуле мы считаем квадрат разности предсказаний объясняющей модели $g(z^{(i)})$ и исходной модели $f(h(z^{(i)}))$, и считаем взвешенную сумму по обучающей выборке, используя веса $w^{(i)}$. Кроме того мы прибавляем штраф за сложность объясняющей модели $\\Omega(g)$.\n",
    "\n",
    "### Резюме\n",
    "\n",
    "Суть подхода LIME в том, что мы аппроксимируем предсказание модели $f$ в окрестности тестового примера $x_0$ более простой, легко интерпретируемой моделью $g$, которая использует упрощенное представление $z$. Например, если модель $g$ линейна, то каждому изменению $\\Delta_i$ (например, суперпикселю в изображении) сопоставляется некий вес. \n",
    "\n",
    "При этом мы надеемся, что такая аппроксимация адекватна, то есть наличие $i$-го изменения линейно влияет на предсказание модели $f$. В некоторых случаях это может оказаться совсем не так, и модель $g$ не сможет хорошо обучиться (функция потерь остенется высокой). Например, в случае изображений это может означать, что мы не можем линейно влиять на предсказание модели, удаляя отдельные суперпиксели. Возможно, модель ориентируется не на отдельные объекты, а на цвет изображения в целом. Тогда можно попробовать использовать другое упрощенное представление, элементами которого является информация об усредненном цвете изображения.\n",
    "\n",
    "Работа алгоритма LIME не зависит от вида модели $f$ (нейронная сеть, решающие деревья и т. д.) и никак явно не использует информацию о том, как модель устроена \"изнутри\", то есть LIME является \"model-agnostic\" алгоритмом интерпретации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1b8f8-78dc-4781-8567-c9ff1232b278",
   "metadata": {},
   "source": [
    "### Примеры и обсуждение\n",
    "\n",
    "На *рис. 2* мы видим объяснение предсказания сверточной нейронной сети Inception ([Szegedy et al., 2014]($Going Deeper with Convolutions$)). Сначала мы рассматриваем выходной нейрон, соответствующий классу \"Electric guitar\", и пытаемся аппроксимировать значение на этом нейроне с помощью линейной модели $g$, которая использует информацию о наличии или отстутствии суперпикселей ($M$ бинарных признаков, где $M$ - количество суперпикселей). В результате для каждого суперпикселя мы получаем вес, то есть вклад этого суперпикселя в предсказание \"Electric guitar\", и выделяем суперпиксели с наибольшим весом. Далее повторяем тот же алгоритм для двух других выходных нейронов, соответствующих классам \"Acoustic guitar\" и \"Labrador\". Как можно видеть из примера, алгоритм LIME концептуально достаточно прост."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac384a19-b52e-4426-b13d-ea84dc7ecca0",
   "metadata": {},
   "source": [
    "<img src=\"assets/lime.jpg\" width=\"800\" align=\"center\">\n",
    "\n",
    "<center><i>Рис. 2. Пример результатов, полученных с помощью метода LIME. Отмечены суперпиксели с наибольшими весами в линейной объясняющей модели.</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e68ba6-bbba-42aa-9e4a-7569a7608bd7",
   "metadata": {},
   "source": [
    "Работа алгоритма LIME определяется выбором набора изменений $\\Delta_i$, этот выбор осуществляется вручную и в каких-то случаях может быть неудачным. Поэтому LIME можно рассматривать как метод, в котором мы сначала формулируем гипотезу о том, как можно было бы объяснить предсказание модели, а затем проверяем ее.\n",
    "\n",
    "Интересно было бы попробовать применить алгоритм LIME для интерпретации предсказаний человека на различных задачах. Такой подход мог бы дать лучшее понимание плюсов, минусов и границ применимости алгоритма.\n",
    "\n",
    "Один из возможных минусов заключается в том, что измененные примеры $x_0 + \\Delta x$ могут быть неестественными, ненатуральными (например, изображение, в котором стерта часть суперпикселей). Модель $f$, напротив, обучалась только на натуральных примерах, и чаще всего от нее не требуется корректная работа на ненатуральных примерах (не лежащих в многообразии исходных данных, в терминологии из [Fefferman et al., 2013]($Testing the Manifold Hypothesis$)).\n",
    "\n",
    "Конечно хотелось бы проверять работу $f$ только для тех $x_0 + \\Delta x$, которые также выглядят натурально. В табличных данных для более надежной интерпретации можно искать натуральные примеры, похожие на $x_0$ (если датасет имеет достаточный размер) и изучать работу модели на этих примерах (то есть сопоставлять изменение в предсказании модели и изменение входных данных относительно $x_0$.\n",
    "\n",
    "В компьютерном знении нас может интересовать, каким образом модель предсказывает на изображении собаку. Вместо удаления случайных суперпикселей (что сделает изображение ненатуральным) можно изучить работу модели на видео, в котором переход между кадрами соответствует переходам между разными натуральными изображениями. Можно даже записать видео, по очереди закрывая отдельные части интересующего нас объекта. Также можно попробовать использовать компьютерную графику для генерации натуральных изображений, хотя такой подход сложен и может повлечь много новых проблем.\n",
    "\n",
    "Конечно, можно разрабатывать более эффективные способы интерпретации для конкретных предметных областей, но особенность LIME именно в том, что это очень общий подход, который может быть применен к широкому классу моделей. При этом многие детали в нем, в частности вид упрощенного представления, могут быть выбраны произвольно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c47e5-22df-4134-9ed2-ea11379f3229",
   "metadata": {},
   "source": [
    "### LIME-SP для глобальной интерпретации моделей\n",
    "\n",
    "Авторы также предлагают надстройку над алгоритмом LIME, называемую submodular pick (SP), которая может помочь интерпретировать модель в целом, а не только на конкретном примере. Для этого выбирается набор тестовых примеров, и каждый пример интерпретируется алгоритмом LIME, при этом к каждому примеру мы применяем одни и те же по смыслу изменения. Используя линейную модель $g$, в результате мы получаем матрицу $W$, строкой которой является номер примера, столбцом - позиция в векторе упрощенного представления, значением - вес данной позиции на данном примере.\n",
    "\n",
    "Например, в случае модели, работающей с текстом и использующей bag-of-words, столбец матрицы $W$ будет соответствовать номер слова в словаре. Однако с суперпикселями так не получится, поскольку на каждом тестовом примере суперпиксели разные по смыслу.\n",
    "\n",
    "Получив матрицу $W$, мы можем рассчитать глобальный вес каждой позиции, найдя норму каждого столбца. Также мы можем попытаться выбрать небольшой набор строк матрицы $W$ такой, чтобы в этом наборе для каждого столбца хотя бы раз встретилось большое значение. Этот набор строк соответствует набору тестовых примеров, которых предположительно может быть достаточно для глобальной интерпретации модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c59ea-6be7-4586-9070-5f7c55446f96",
   "metadata": {},
   "source": [
    "## SHAP: Shapley Additive Explanation Values\n",
    "\n",
    "В данном разделе мы рассмотрим подход SHAP ([Lundberg and Lee, 2017]($A Unified Approach to Interpreting Model Predictions$)), позволяющий оценивать важность признаков в произвольных моделях машинного обучения, а также может быть применен как частный случай метода LIME.\n",
    "\n",
    "### Shapley values в теории игр\n",
    "\n",
    "[Теория игр](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B8%D0%B3%D1%80) - это область математики, изучающей взаимодействие (игру) между игроками, преследующими некие цели и действующими по неким правилам. [Кооперативной игрой](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B8%D0%B3%D1%80) называется такая игра, в которых группа игроков (*коалиция*) действует совместно. С середины XX века ([Shapley, 1952]($A Value for N-Person Games$)) известны так называмые [Shapley values](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80_%D0%A8%D0%B5%D0%BF%D0%BB%D0%B8), которые позволяют численно оценить вклад каждого игрока в достижение общей цели.\n",
    "\n",
    "Пусть существует *характеристическая функция* (отображение из множества игроков в число), которая описывает эффективность данной коалиции игроков, действующей совместно. Тогда Shapley value для каждого игрока - это число, рассчитываемое по достаточно простой формуле. Обозначим за $\\Delta(i, S)$ прирост эффективности от добавления игрока $i$ в коалицию игроков $S$. Пусть всего есть $N$ игроков. Рассмотрим множество $\\Pi$ всех возможных упорядочиваний игроков, и обозначим за $(\\text{players before } i \\text{ in } \\pi)$ множество игроков, стоящих перед игроков $i$ в упорядочивании $\\pi$. Shapley value для игрока $i$ рассчитывается таким образом:\n",
    "\n",
    "$\\phi(i) = \\cfrac{1}{N!} \\sum\\limits_{\\pi \\in \\Pi} \\Delta(i, (\\text{players before } i \\text{ in } \\pi)) \\tag{1}$\n",
    "\n",
    "То есть мы считаем средний прирост эффективности от добавления $i$-го игрока в коалицию игроков, стоящих перед ним, по всем возможным упорядочиваниям игроков (количество элементов суммы равно $N!$). Данная формула задается аксиоматически, то есть формулируется ряд необходимых свойств и доказывается, что существует единственное решение, которое им удовлетворяет. Поскольку $\\Delta(i, S)$ не зависит от порядка игроков в $S$, то в формуле $(1)$ можно объединить равные друг другу слагаемые и переписать ее в следующем виде:\n",
    "\n",
    "$\\phi(i) = \\sum\\limits_{S \\subseteq \\{1, 2, \\dots, N\\} \\setminus i} \\cfrac{|S|! (|N| - |S| - 1)!}{N!}\\ \\Delta(i, S) \\tag{2}$\n",
    "\n",
    "Данная формула является взвешенной суммой, в которой веса принимают наибольшие значения при $|S| \\approx 0$ или $|S| \\approx |N|$ и наименьшие значения при $|S| \\approx |N|\\ /\\ 2$. Подробнее см. также [Kumar et al., 2020]($Problems with Shapley-value-based explanations as feature importance measures$), раздел 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5478de-f97a-4282-86a7-706f4d057e42",
   "metadata": {},
   "source": [
    "### Shapley regression values\n",
    "\n",
    "Тот же подход можно применить в машинном обучении, если игроками считать наличие отдельных признаков, а результатом игры - ответ модели на конкретном примере $x$. Shapley values применяются в машинном обучении еще с XX века ([Kruskal, 1987]($Relative Importance by Averaging Over Orderings$)).\n",
    "\n",
    "> Game-theoretic formulations of feature importance have become popular as a way to \"explain\" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique\n",
    "Shapley values. ... In this setting, the \"players\" are the features used by the model, and the game is the prediction of the model. ([Kumar et al., 2020]($Problems with Shapley-value-based explanations as feature importance measures$))\n",
    "\n",
    "Shapley regression values ([Lipovetsky and Conklin, 2001]($Analysis of Regression in Game Theory Approach$)) позволяют оценить вклад каждого признака в ответ модели $f$. Зафиксируем конкретный тестовый пример $x$, и за $\\Delta(i, S)$ будем считать изменение в предсказании $x$ между моделью $f_{S \\cup \\{i\\}}$, обученной на признаках $S \\cup \\{i\\}$ и моделью $f_S$, обученной на признаках $S$:\n",
    "\n",
    "$\\Delta(i, S) = (f_{S \\cup \\{i\\}}(x_{S \\cup \\{i\\}}) - f_S(x_S)) \\tag{3}$\n",
    "\n",
    "Тогда вклад отдельных признаков в величину предсказания модели можно оценивать по формулам $(1)$ и $(2)$. Отметим, что мы рассматриваем не вклад каждого признака в точность модели, а вклад каждого признака в величину предсказания модели на конкретном тестовом примере, что помогает интерпретировать это предсказание. Если модель предназначена для классификации, то в выходными значениями считаются logits - значения до операции softmax/sigmoid.\n",
    "\n",
    "В целом есть некоторые проблемы в представлении признака как игрока. Дело в том, что для игрока существует лишь два состояния: либо он есть, либо его нет. Признак же существует как минимум в трех состояниях: два разных значения и неопределенное значение (отсутствие признака). В Shapley regression values сранивается текущее значение признака на примере $x$ с его полным отсутствием при обучении и тестировании.\n",
    "\n",
    "Минусом такого подхода является высокая сложность вычислений: для расчета Shapley regression values нужно обучать модель на всех возможных подмножествах признаков, что в большинстве случаев невыполнимо. Однако мы можем намного быстрее посчитать приблизительное значение Shapley values, если будем считать не все элементы суммы $(3)$, а лишь некоторые, обладающие большими весами. Для этого мы можем использовать веса как вероятности при семплировании элементов суммы (аппроксимация семплированием, *Shapley sampling values* - [Štrumbelj and Kononenko, 2014]($Explaining prediction models and individual predictions with feature contributions$))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ec3d8-cef1-4b75-9344-011d97d607a5",
   "metadata": {},
   "source": [
    "### SHAP values\n",
    "\n",
    "Можем ли мы аппроксимировать Shapley regression values, обучая всего одну модель на всех признаках? В этом случае нам нужно получать предсказание модели в случаях, когда многие из признаков имеют неопределенные значения. Большинство моделей не умеют работать с такими данными. Как правило модели машинного обучения моделируют условное мат. ожидание $E [y | x]$, взятое по распределению данных $\\mathcal{D}$, иными словами - ожидаемое (среднее) значение $y$ при заданном $x$. Пусть часть признаков известны, часть пропущены. Обозначим за $x_S$ известные признаки, тогда можно считать, что $f(x_S) := E [f(x) | x_S]$. Данная формула означает, что за ответ модели $f$ на примере с пропущенными значениями $x_S$ мы берем мат. ожидание ответа $f$ на примерах $x^\\prime$, взятых из распределения данных $\\mathcal{D}$, таких, что $x^\\prime_S = x_S$. О способе подсчета таких величин мы поговорим далее, но сначала дадим формальное определение SHAP values ([Lundberg and Lee, 2017]($A Unified Approach to Interpreting Model Predictions$)).\n",
    "\n",
    "----\n",
    "\n",
    "**Определение (SHAP values)**. Пусть мы имеем модель $f$, распределение данных $\\mathcal{D}$ и некий тестовый пример $x$ и хотим оценить важность текущих значений каждого признака по сравнению с их неопределенными значениями. SHAP (SHapley Additive exPlanation) values для признаков на примере $x$ - это Shapley values, рассчитываемые для следующей кооперативной игры:\n",
    "\n",
    "- Игроками являются признаки (наличие $i$-го игрока означает текущее значение $i$-го признака на примере $x$, отсутствие $i$-го игрока означает неопределенное значение $i$-го признака - так же, как в Shapley regression values).\n",
    "- Характеристической функцией коалиции признаков $S$ является условное мат. ождидание $E [f(x) | x_S]$ по распределению данных $\\mathcal{D}$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6381965-09cc-4fed-b67c-2f2bfb32b82f",
   "metadata": {},
   "source": [
    "Таким образом, алгоритм расчета SHAP values следует формулам $(1)$ и $(2)$: для каждого возможного упорядочивания признаков мы берем все признаки, стоящие перед $i$-м признаком (обозначим их за $S$) и считаем величину $\\Delta_f(i, S) = E [f(x) | x_{S\\ \\cup\\ i}] - E [f(x) | x_S]$ (о способе подсчета см. далее), после чего усредняем полученные значения по всем упорядочиваниям. Это означает, что SHAP values описывают *ожидаемый прирост выходного значения модели при добавлении $i$-го признака в текущем примере*.\n",
    "\n",
    "> We propose SHAP values as a unified measure of feature importance. These are the Shapley values of a conditional expectation function of the original model. ...  SHAP values attribute to each feature the change in the expected model prediction when conditioning on that feature. ([Lundberg and Lee, 2017]($A Unified Approach to Interpreting Model Predictions$))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0e2a6-7a7a-4f34-b101-f293cfb7e5ce",
   "metadata": {},
   "source": [
    "Наиболее важным свойством, которым обладают Shapley values, рассчитанные по формулам $(1)$ и $(2)$, является *состоятельность (consistency)*. Это свойство означает, что если (зафиксировав пример $x$) мы рассчитываем Shapley values для двух моделей $f$ и $f^\\prime$, и для любых $S$, $i$ верно, что вклад признака $i$ при имеющемся множесте признаков $S$ в первой модели не меньше, чем во второй, то Shapley value этого признака в первой модели также не меньше, чем во второй:\n",
    "\n",
    "$\\textbf{If:}\\ \\ \\forall S, i: \\Delta_f(i, S) \\geq \\Delta_{f^\\prime}(i, S) \\quad \\textbf{Then:}\\ \\ \\phi_f(x) \\geq \\phi_{f^\\prime}(x)$\n",
    "\n",
    "Нужно также отметить несколько важных деталей:\n",
    "1. Определение SHAP не говорит о том, как именно рассчитывать $E [f(x) | x_S]$ в условиях ограниченной по размеру выборки данных. Если бы мы имели прямой доступ к распределению $\\mathcal{D}$, неограниченные вычислительные ресурсы и могли бы бесконечно семплировать из $\\mathcal{D}$, то была бы возможна оценка $E [f(x) | x_S]$ с любой точностью. Однако обычно мы имеем лишь обучающую и тестовую выборки, взятые из $\\mathcal{D}$. Их может быть недостаточно, чтобы надежно оценить $E [f(x) | x_S]$ (см. также [Kumar et al., 2020]($Problems with Shapley-value-based explanations as feature importance measures$), раздел 3.1.1).\n",
    "2. SHAP values в данном определении зависят не только от модели $f$, но и от распределения данных. Это означает, что на разных распределениях данных важность признаков в одной и той же модели может оказаться разной.\n",
    "3. SHAP values в целом не являются универсальным способом интерпретации как минимум потому, что выходные данные могут иметь сложный формат, а входные признаки могут быть не интерпретируемы. О других проблемах SHAP values мы поговорим позже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe459a11-869e-4987-ba2c-170e074a69d3",
   "metadata": {},
   "source": [
    "### Linear SHAP values\n",
    "\n",
    "Задачу оценки $E [f(x) | x_S]$, можно принципиально упростить, если [предположить](https://en.wikipedia.org/wiki/Statistical_assumption), что наличие каждого признака линейно и независимо влияет на ответ модели. Тогда значения признаков, не входящих в $S$, не зависят от $x_S$ и друг от друга и линейно влияют на ответ, и $E [f(x) | x_S]$ можно рассчитать, заменив пропущенные значения их мат. ожиданиями, которые можно приблизить средним значением по выборке данных. Обозначив пропущенные признаки за $\\overline{S}$, получим\n",
    "\n",
    "$f(x_S) := E [f(x) | x_S] \\approx f([x_S, E[x_\\overline{S}]])$\n",
    "\n",
    "> feature independence and model linearity are two optional assumptions simplifying the computation of the expected values ([Lundberg and Lee, 2017]($A Unified Approach to Interpreting Model Predictions$))\n",
    "\n",
    "В формуле стоит знак приблизительного равенства потому, что линейность и независимость - это лишь предположения: чем ближе они к истине, тем точнее аппроксимация. С помощью данной формулы мы можем рассчитать $f(x_S)$ для любого подмножества признаков $x_S$, а значит получили возможность рассчитывать SHAP values на практике по формуле $(2)$, опционально применяя также аппроксимацию семплированием.\n",
    "\n",
    "Не является ли линейность и независимость влияния признаков чрезмерно грубым упрощением? В некоторых случаях возможно является, но вспомним, что в методе LIME мы использовали такое же допущение, обучая линейную объясняющую модель $g$. О связи SHAP и LIME мы поговорим далее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bee5ca-6af6-4869-bd66-1af842df20f0",
   "metadata": {},
   "source": [
    "### Shapley kernel в LIME\n",
    "\n",
    "Авторы сопоставляют SHAP с несколькими появившимися в 2010-х годах методами интерпретации моделей машинного обучения, которые используют локальную аппроксимацию, в первую очередь с LIME, а также с *layer-wise relevance propagation* ([Bach et al., 2015]($On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation$)) и *activation difference propagation (DeepLIFT)* ([Shrikumar et al., 2017]($Learning Important Features Through Propagating Activation Differences$)), которые мы сейчас не будем рассматривать.\n",
    "\n",
    "Как LIME, так и DeepLIFT рассматривают локальное упрощенное представление в виде вектора $z$ (где $z_i = 0$ означает наличие изменения $\\Delta_i$ относительно примера $x_0$). В разделе, посвященном LIME, мы говорили о таком представлении и об объясняющей модели $g$, аппроксимирующей $f(h(z))$. Авторы SHAP рассматривают случай, когда объясняющая модель линейна (такие способы интерпретации авторы называют additive feature attribution methods):\n",
    "\n",
    "$g(z) = \\phi_0 + \\sum\\limits_{i=1}^M \\phi_i z_i, \\quad \\phi_i \\in \\mathbb{R}, z_i \\in \\{0, 1\\} \\tag{4}$\n",
    "\n",
    "Важным является случай, когда $\\Delta_i$ означает удаление некоего признака из $x_0$, то есть при $z_i = 0$ один из признаков в $x_0$ заменяется на неопределенное значение. В этом случае будем говорить, что $z_i$ *задает наличие признака*. При этом постановка задачи становится эквивалентой той, на которой основан SHAP. Если при этом модель $g$ линейна, то мы можем использовать linear SHAP values, то есть заменять отсутствующие признаки их средним значением.\n",
    "\n",
    "Вспомним, что в LIME при выборе линейной модели $g$ мы затем искали ее веса $\\phi_i$, но для этого мы должны были выбрать функцию потерь $\\mathcal{L}$, метрику сходства $\\pi$ и штраф сложности $\\Omega$. Авторы SHAP выводят единственно возможные значения для $\\mathcal{L}$, $\\pi$ и $\\Omega$ такие, что полученные веса $\\phi_i$ будут равны SHAP values, рассчитанным по формуле $(2)$, при обучении на всех возможных $z$. Если же мы будем обучать $g$ не на всех $z$, то полученные веса будут аппроксимировать $\\phi_i(f, x_0)$.\n",
    "\n",
    "- $\\mathcal{L}$ - среднеквадратичное отклонение\n",
    "- $\\Omega(g) = 0$\n",
    "- $\\pi(z) = \\cfrac{M-1}{{{M}\\choose{|z|}}\\ |z|\\ (M-|z|)}$, где ${M}\\choose{|z|}$ - [биномиальный коэффициент](https://en.wikipedia.org/wiki/Binomial_coefficient)\n",
    "\n",
    "Основное отличие от примера, предложенного авторами LIME ([Ribeiro et al., 2016]($Why Should I Trust You?: Explaining the Predictions of Any Classifier$)), состоит именно в $\\pi(z)$. Как мы видели в разделе \"Объясняющая модель\", $\\pi(z)$ фактически является метрикой сходства между векторами, такие метрики часто называют ядрами (kernels), поэтому предложенная формула для $\\pi(z)$ получила название **Shapley kernel**.\n",
    "\n",
    "Shapley kernel назначает большие веса примерам как с большим количеством единиц, так и с большим количеством нулей.  Это означает, что мы обращаем основное внимание на примеры с (почти) максимальным и (почти) минимальным количеством компонентов (игроков). На *рис. 3* показано отличие Shapley kernel от L2 distance и cosine similarity, которые предлагалось использовать в LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7567afe-ee4c-4491-afed-c20315217764",
   "metadata": {},
   "source": [
    "<img src=\"assets/shap.jpg\" width=\"400\" align=\"center\">\n",
    "\n",
    "<center><i>Рис. 3. Shapley kernel в сравнении с L2 distance и cosine similarity.</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6832f-b9f8-4370-843c-08cdd7fcb0d8",
   "metadata": {},
   "source": [
    "### TreeExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c95908-63e7-4be6-a1f5-2bce91a33d6c",
   "metadata": {},
   "source": [
    "## SHAP на практике\n",
    "\n",
    "Для подсчета SHAP values существует [python-библиотека SHAP](https://github.com/slundberg/shap), которая может работать со многими ML-моделями (XGBoost, CatBoost, TensorFlow, scikit-learn и др) и имеет документацию с большим количеством примеров. С помощью бибилиотеки SHAP можно строить различные схемы и графики, описывающие важность признаков в модели и их влияние на ответ. Рассмотрим примеры из документации.\n",
    "\n",
    "На *рис. 4* показана [waterfall](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/waterfall.html)-схема, объясняющая предсказание на первом тестовом примере из датасета Boston housing. Схема читается снизу вверх, и признаки упорядочены по возрастанию их SHAP values. Например, SHAP value -0.43 для признака CRIM (имеющего значение 0.006) говорит о том, что \n",
    "\n",
    "> The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output *over the background dataset* to the model output for this prediction.\n",
    "\n",
    "TODO kernel shap vs tree shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ac7f1-11df-430c-aa31-78a3c5cae93d",
   "metadata": {},
   "source": [
    "<img src=\"assets/shap2.png\" width=\"500\" align=\"center\">\n",
    "\n",
    "<center><i>Рис. 4. Waterfall-схема.</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a20967-283a-446b-8271-dd5f5a8442a7",
   "metadata": {},
   "source": [
    "## Проблемы и модификации SHAP: обзор последующих работ\n",
    "\n",
    "Метод SHAP основан на математической базе с аксиомами и доказательствами, однако это еще не говорит о том, что SHAP является наилучшим методом оценки важности признаков, поскольку предположение о линейности и независимости признаков в окрестности тестового примера может быть чрезмерно грубым упрощением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e81fb-08e3-485a-b5a2-43147a43f338",
   "metadata": {},
   "source": [
    "SHAP на практике\n",
    "\n",
    "https://arxiv.org/pdf/2002.11097.pdf\n",
    "\n",
    "https://papers.nips.cc/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf\n",
    "\n",
    "https://papers.nips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf\n",
    "\n",
    "https://arxiv.org/pdf/2010.14592.pdf\n",
    "\n",
    "https://www.connectedpapers.com/main/364f02eff4f10ea602d86fd8c98c8694b76f46fd/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ba9a7-63b4-44df-b0c5-9439aaa8734a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
