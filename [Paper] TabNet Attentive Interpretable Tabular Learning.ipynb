{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795d4686-57c9-4a28-aac3-67086238dac5",
   "metadata": {},
   "source": [
    "В данной статье авторы описывают нейросетевую архитектуру TabNet ... (**TODO**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88cae0-0486-4bb6-9fc3-fbdaec9f3d52",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "Во введении кратко рассматривается вопрос о том, почему решающие деревья (DT) часто оказываются лучше нейронных сетей (DNN) на табличных данных. Авторы делают ряд утверждений, но впрочем не приводят к ним пояснений или доказательств:\n",
    "\n",
    "> DT-based approaches ... are representionally efficient for decision manifolds with approximately hyperplane boundaries which are common in tabular data.\n",
    "\n",
    "Это утверждение не вполне понятно: чтобы реализовать зависимость в виде гиперплоскости $y = ax_1 + bx_2 + c$, нейронной сети достаточно всего трех параметров, тогда как решающее дерево потребует множества параметров, чтобы хоть как-то приблизить эту зависимость.\n",
    "\n",
    "> Previously-proposed DNN architectures are not well-suited for tabular data: e.g. stacked convolutional layers or multi-layer perceptrons (MLPs) are *vastly overparametrized* – the lack of appropriate inductive bias often causes them to fail to find optimal solutions for tabular decision manifolds (Goodfellow, Bengio, and Courville\n",
    "2016; Shavitt and Segal 2018; Xu et al. 2019).\n",
    "\n",
    "Здесь я тоже не до конца понимаю авторов. Наличие чрезмерной параметризации само по себе еще не означает, что модель плохая: это еще зависит от конкретного вида модели и алгоритма обучения. Например, нейронные сети бесконечной ширины (см. [Kernel Methods for Deep Learning]($Kernel Methods for Deep Learning$)) вполне могут хорошо обобщаться, несмотря на бесконечное число параметров. Кроме того, недавно был обнаружен эффект Deep Double Descent ([Nakkiran et al., 2019]($Deep Double Descent: Where Bigger Models and More Data Hurt$)), в котором тоже показывается, что при чрезмерной параметризации модель может хорошо обобщаться.\n",
    "\n",
    "Мотивация к изучению методов глубокого обучения на табличных данных у авторов следующая:\n",
    "\n",
    "1. Методы глубокого обучения хорошо масштабируются при увеличении объема обучающих данных ([Hestness et al., 2017]($Deep Learning Scaling is Predictable, Empirically$)). Хотя, с другой стороны, это же может быть справедливо и для градиентного бустинга.\n",
    "2. Глубокое обучение открывает возможности для обучения представлений (representation learning), обучения на неразмеченных данных и переноса обучения (типичным примером является файн-тюнинг предобученных нейронных сетей в CV и NLP).\n",
    "3. Благодаря end-to-end обучению можно совмещать табличные данные с изображениями и текстом.\n",
    "4. Согласно авторам, модели глубокого обучения можно обучать на потоковых данных (learning from streaming data), хотя это же может быть справедливо и для градиентного бустинга.\n",
    "5. Согласно авторам, в глубоком обучении снижается необходимость в feature engineering, по сравнению с решающими деревьями (хотя мне это не до конца понятно, скорее верно обратное, ведь монотонные преобразования признаков играют большую роль с нейронных сетях и практически не играют роли в решающих деревьях)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6c3df-6b6f-4bff-b303-91ce68c8a2e3",
   "metadata": {},
   "source": [
    "### Связанные работы\n",
    "\n",
    "Говоря о чрезмерной параметризации нейронных сетей, авторы ссылаются на работу *Regularization Learning Networks: Deep Learning for Tabular Datasets* ([Shavitt and Segal, 2018]($Regularization Learning Networks: Deep Learning for Tabular Datasets$)). Авторы этой статьи высказывают гипотезу о том, что преимущество решающих деревьев перед нейронными сетями на табличных данных связано с тем, что в табличных данных разные признаки имеют разную степень важности. Из этого каким-то образом авторы заключают, что нужно вводить для каждого веса свой регуляризующий коэффициент (например, для L2-регуляризации).\n",
    "\n",
    "> We hypothesized that this potentially large variability in the relative importance of different input features may partly explain the lower performance of DNNs on such tabular datasets. One way to overcome this limitation could be to assign a different regularization coefficient to every weight, which might allow the network to accommodate the non-distributed representation and the variability in relative importance found in tabular datasets.\n",
    "\n",
    "Авторы предлагают параллельно с обучением весов сети обучать коэффициенты регуляризации на каждом весе с помощью минимизации специальной дополнительной функции потерь, которая равна функции потерь предсказания (например, MSE или кроссэнтропии) на селдующем батче, из которой градиент распространяется в текущий батч. Таким образом, мы подталкиваем коэффициенты регуляризации к тому, чтобы они минимизировали функцию потерь предсказания на следующем батче.\n",
    "\n",
    "Таким образом, в статье описана мысль о собственном регуляризующем коэффициенте для каждого веса сети. Это похоже на метод automatic relevance determination ([Neal, 1995]($Bayesian Learning for Neural Networks$), см. также [Schmidt, 2018](https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L32.pdf)), где аналогичный подход адаптивной регуляризации применяется для отбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96749f27-fa58-4379-9bab-898caeb02a1a",
   "metadata": {},
   "source": [
    "### Функция Sparsemax\n",
    "\n",
    "В архитектуре TabNet применяется функция активации Sparsemax, которую стоит рассмотреть в отдельном разделе. \n",
    "\n",
    "Sparsemax ([Martins and Astudillo, 2016]($From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification$)) является альтернативой Softmax, принимает на вход вектор логитов $z \\in \\mathbb{R}^N$ и возвращает вектор вероятностей $p \\in \\mathbb{R}^N$ такой, что сумма всех вероятностей равна единице. Sparsemax определяется следующим образом. Обозначим за $\\Delta^{N-1}$ множество всех возможных распределений вероятностей на $N$ классах (то есть подмножество $\\mathbb{R}^N$, состоящее из векторов, в которых каждый элемент неотрицателен и сумма всех элементов равна единице). Тогда $\\text{Sparsemax}(z)$ - это ближайший к $z$ элемент из множества $\\Delta^{N-1}$:\n",
    "\n",
    "$\\text{Sparsemax}(z) := \\underset{p \\in \\Delta^{N-1}}{\\text{argmin}} \\| z - p \\|_2$\n",
    "\n",
    "На рисунке ниже отмечено множество $\\Delta^1$ в пространстве $\\mathbb{R}^2$ и показаны три примера, где красной точкой отмечен вектор логитов $z$, а синей точкой показан вектор вероятностей $\\text{Sparsemax}(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594c9af-3a50-4f94-ad96-9eb9c1120831",
   "metadata": {},
   "source": [
    "<img src=\"assets/sparsemax.png\" width=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4679d7-01c0-42c0-b3f2-d4c2c746e8c8",
   "metadata": {},
   "source": [
    "Как на практике считать $\\text{Sparsemax}(z)$? Множество $\\Delta^{N-1}$ называется симплексом и представляет собой участок $(N-1)$-мерной плоскости в $N$-мерном пространстве. При этом существует такое число $\\tau(z)$, что:\n",
    "\n",
    "$\\text{Sparsemax}(z) = \\text{ReLU}(z - \\tau(z))$\n",
    "\n",
    "Где операция $\\text{ReLU}$ означает зануление отрицательных элементов вектора, а отнятие числа от вектора означает отнятие его от каждого элемента вектора (нормалью к $\\Delta^{n-1}$ является вектор, состоящий из единиц). Таким образом, операцию Sparsemax можно представить себе как равномерное уменьшение всех положительных элементов в векторе логитов до тех пор, пока их сумма не станет равной единице.\n",
    "\n",
    "Существует несложный аналитический алгоритм нахождения $\\tau(z)$ (см. [Martins and Astudillo, 2016]($From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification$), Algorithm 1). Важно отметить, что $\\tau(z)$ не эквивалентно расстоянию от $z$ до плоскости, на которой лежит множество $\\Delta^{n-1}$.\n",
    "\n",
    "Для обратного распространения ошибки нам нужно уметь считать матрицу производных (Якобиан) выходов Sparsemax по входным данным. Обозначим за $S(z)$ множество индексов таких, что $\\text{Sparsemax}(z)_i > 0$. Якобиан функции Sparsemax считается следующим образом:\n",
    "\n",
    "$\n",
    "\\cfrac{\\partial \\text{Sparsemax}(z)_i}{\\partial z_j} = \n",
    "\\begin{cases}\n",
    "    1 - 1/|S(z)| & \\text{if } i, j \\in S(z), i = j\\\\\n",
    "    - 1/|S(z)| & \\text{if } i, j \\in S(z), i \\neq j\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11f763-cc61-4cea-bb69-557050e58ee0",
   "metadata": {},
   "source": [
    "Таким образом, в случае нейронной сети градиент функции потерь не будет действовать на те нейроны, на которых после Sparsemax получается нулевая вероятность (производная по этим нейронам будет равна нулю)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672cd14-a7b2-44da-8c39-de40a9d75c90",
   "metadata": {},
   "source": [
    "### Архитектура TabNet\n",
    "\n",
    "Решающие деревья обучаются выбирать наиболее статистически значимые признаки на каждом шаге их построения. TabNet следует приблизительно той же идее, но в TabNet *для каждого примера выбирается свое подмножество релевантных признаков* в ходе прямого прохода по нейронной сети.\n",
    "\n",
    "Еще одна особенность TabNet заключается в том, что она способна обучаться на частично размеченных или вовсе неразмеченных данных. Шаг обучения на неразмеченных данных заключается в том, что модель пытается восстановить значения признаков, закрытые маской (по аналогии с задачей masked language modeling в [BERT]($GPT и BERT$)).\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
