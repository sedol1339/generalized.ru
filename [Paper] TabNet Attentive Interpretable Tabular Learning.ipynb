{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795d4686-57c9-4a28-aac3-67086238dac5",
   "metadata": {},
   "source": [
    "В данной статье авторы описывают нейросетевую архитектуру TabNet ... (**TODO**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88cae0-0486-4bb6-9fc3-fbdaec9f3d52",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "Во введении кратко рассматривается вопрос о том, почему решающие деревья (DT) часто оказываются лучше нейронных сетей (DNN) на табличных данных. Авторы делают ряд утверждений, но впрочем не приводят к ним пояснений или доказательств:\n",
    "\n",
    "> DT-based approaches ... are representionally efficient for decision manifolds with approximately hyperplane boundaries which are common in tabular data.\n",
    "\n",
    "Это утверждение не вполне понятно: чтобы реализовать зависимость в виде гиперплоскости $y = ax_1 + bx_2 + c$, нейронной сети достаточно всего трех параметров, тогда как решающее дерево потребует множества параметров, чтобы хоть как-то приблизить эту зависимость.\n",
    "\n",
    "> Previously-proposed DNN architectures are not well-suited for tabular data: e.g. stacked convolutional layers or multi-layer perceptrons (MLPs) are *vastly overparametrized* – the lack of appropriate inductive bias often causes them to fail to find optimal solutions for tabular decision manifolds (Goodfellow, Bengio, and Courville\n",
    "2016; Shavitt and Segal 2018; Xu et al. 2019).\n",
    "\n",
    "Здесь я тоже не до конца понимаю авторов. Наличие чрезмерной параметризации само по себе еще не означает, что модель плохая: это еще зависит от конкретного вида модели и алгоритма обучения. Например, нейронные сети бесконечной ширины (см. [Kernel Methods for Deep Learning]($Kernel Methods for Deep Learning$)) вполне могут хорошо обобщаться, несмотря на бесконечное число параметров. Кроме того, недавно был обнаружен эффект Deep Double Descent ([Nakkiran et al., 2019]($Deep Double Descent: Where Bigger Models and More Data Hurt$)), в котором тоже показывается, что при чрезмерной параметризации модель может хорошо обобщаться.\n",
    "\n",
    "Мотивация к изучению методов глубокого обучения на табличных данных у авторов следующая:\n",
    "\n",
    "1. Методы глубокого обучения хорошо масштабируются при увеличении объема обучающих данных ([Hestness et al., 2017]($Deep Learning Scaling is Predictable, Empirically$)). Хотя, с другой стороны, это же может быть справедливо и для градиентного бустинга.\n",
    "2. Глубокое обучение открывает возможности для обучения представлений (representation learning), обучения на неразмеченных данных и переноса обучения (типичным примером является файн-тюнинг предобученных нейронных сетей в CV и NLP).\n",
    "3. Благодаря end-to-end обучению можно совмещать табличные данные с изображениями и текстом.\n",
    "4. Согласно авторам, модели глубокого обучения можно обучать на потоковых данных (learning from streaming data), хотя это же может быть справедливо и для градиентного бустинга.\n",
    "5. Согласно авторам, в глубоком обучении снижается необходимость в feature engineering, по сравнению с решающими деревьями (хотя мне это не до конца понятно, скорее верно обратное, ведь монотонные преобразования признаков играют большую роль с нейронных сетях и практически не играют роли в решающих деревьях)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6c3df-6b6f-4bff-b303-91ce68c8a2e3",
   "metadata": {},
   "source": [
    "### Связанные работы\n",
    "\n",
    "Говоря о чрезмерной параметризации нейронных сетей, авторы ссылаются на работу *Regularization Learning Networks: Deep Learning for Tabular Datasets* ([Shavitt and Segal, 2018]($Regularization Learning Networks: Deep Learning for Tabular Datasets$)). Авторы этой статьи высказывают гипотезу о том, что преимущество решающих деревьев перед нейронными сетями на табличных данных связано с тем, что в табличных данных разные признаки имеют разную степень важности. Из этого каким-то образом авторы заключают, что нужно вводить для каждого веса свой регуляризующий коэффициент (например, для L2-регуляризации).\n",
    "\n",
    "> We hypothesized that this potentially large variability in the relative importance of different input features may partly explain the lower performance of DNNs on such tabular datasets. One way to overcome this limitation could be to assign a different regularization coefficient to every weight, which might allow the network to accommodate the non-distributed representation and the variability in relative importance found in tabular datasets.\n",
    "\n",
    "Авторы предлагают параллельно с обучением весов сети обучать коэффициенты регуляризации на каждом весе с помощью минимизации специальной дополнительной функции потерь, которая равна функции потерь предсказания (например, MSE или кроссэнтропии) на селдующем батче, из которой градиент распространяется в текущий батч. Таким образом, мы подталкиваем коэффициенты регуляризации к тому, чтобы они минимизировали функцию потерь предсказания на следующем батче.\n",
    "\n",
    "Таким образом, в статье описана мысль о собственном регуляризующем коэффициенте для каждого веса сети. Это похоже на метод automatic relevance determination ([Neal, 1995]($Bayesian Learning for Neural Networks$), см. также [Schmidt, 2018](https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L32.pdf)), где аналогичный подход адаптивной регуляризации применяется для отбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672cd14-a7b2-44da-8c39-de40a9d75c90",
   "metadata": {},
   "source": [
    "### Отбор признаков\n",
    "\n",
    "Отбор признаков может быть глобальным или индивидуальным (instance-wise). Индивидуальный отбор признаков означает, что для каждого примера выбирается свое подмножество релевантных признаков. Такой подход ранее применялся для интерпретации обученных моделей ([Chen et al., 2018]($Learning to Explain: An Information-Theoretic Perspective on Model Interpretation$)).\n",
    "\n",
    "Принцип работы TabNet во многом основан на индивидуальном отборе признаков. Решающие деревья обучаются выбирать наиболее статистически значимые признаки на каждом шаге, и архитектура TabNet следует той же идее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c2c70-9b25-48b7-bbb7-6b6a6c272caf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
