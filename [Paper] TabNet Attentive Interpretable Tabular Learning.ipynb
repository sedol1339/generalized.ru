{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795d4686-57c9-4a28-aac3-67086238dac5",
   "metadata": {},
   "source": [
    "В данной статье авторы ... (**TODO**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88cae0-0486-4bb6-9fc3-fbdaec9f3d52",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "Во введении кратко рассматривается вопрос о том, почему решающие деревья (DT) часто оказываются лучше нейронных сетей (DNN) на табличных данных. Авторы делают ряд утверждений, но впрочем не приводят к ним никаких пояснений или доказательств:\n",
    "\n",
    "> DT-based approaches ... are representionally efficient for decision manifolds with approximately hyperplane boundaries which are common in tabular data.\n",
    "\n",
    "Это утверждение не вполне понятно: чтобы реализовать зависимость в виде гиперплоскости $y = ax_1 + bx_2 + c$, нейронной сети достаточно всего трех параметров, тогда как решающее дерево потребует множества параметров, чтобы хоть как-то приблизить эту зависимость.\n",
    "\n",
    "> Previously-proposed DNN architectures are not well-suited for tabular data: e.g. stacked convolutional layers or multi-layer perceptrons (MLPs) are *vastly overparametrized* – the lack of appropriate inductive bias often causes them to fail to find optimal solutions for tabular decision manifolds (Goodfellow, Bengio, and Courville\n",
    "2016; Shavitt and Segal 2018; Xu et al. 2019).\n",
    "\n",
    "Здесь я тоже не до конца понимаю авторов. Наличие чрезмерной параметризации само по себе еще не означает, что модель плохая: это еще зависит от конкретного вида модели и алгоритма обучения. Например, нейронные сети бесконечной ширины (см. [Kernel Methods for Deep Learning]($Kernel Methods for Deep Learning$)) вполне могут хорошо обобщаться, несмотря на бесконечное число параметров. Кроме того, недавно был обнаружен эффект Deep Double Descent ([Nakkiran et al., 2019]($Deep Double Descent: Where Bigger Models and More Data Hurt$)), в котором тоже показывается, что при чрезмерной параметризации модель может хорошо обобщаться.\n",
    "\n",
    "### Связанные работы\n",
    "\n",
    "Говоря о чрезмерной параметризации нейронных сетей, авторы ссылаются на работу *Regularization Learning Networks: Deep Learning for Tabular Datasets* ([Shavitt and Segal, 2018]($Regularization Learning Networks: Deep Learning for Tabular Datasets$)). Авторы этой статьи высказывают гипотезу о том, что преимущество решающих деревьев перед нейронными сетями на табличных данных связано с тем, что в табличных данных разные признаки имеют разную степень важности. Из этого каким-то образом авторы заключают, что нужно вводить для каждого веса свой регуляризующий коэффициент (например, для L2-регуляризации).\n",
    "\n",
    "Авторы предлагают параллельно с обучением весов сети обучать коэффициенты регуляризации на каждом весе с помощью минимизации специальной дополнительной функции потерь, которая равна функции потерь предсказания (например, MSE или кроссэнтропии) на селдующем батче, из которой градиент распространяется в текущий батч. Таким образом, мы подталкиваем коэффициенты регуляризации к тому, чтобы они минимизировали функцию потерь предсказания на следующем батче.\n",
    "\n",
    "Таким образом, в статье описана мысль о собственном регуляризующем коэффициенте для каждого веса сети. Это похоже на метод automatic relevance determination ([Neal, 1995]($Bayesian Learning for Neural Networks$), см. также [Schmidt, 2018](https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L32.pdf)), где аналогичный подход адаптивной регуляризации применяется для отбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6c3df-6b6f-4bff-b303-91ce68c8a2e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
