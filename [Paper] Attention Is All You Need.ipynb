{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854a0928-20a4-49d7-b445-3f5c2d02f263",
   "metadata": {},
   "source": [
    "В данной статье описана архитектура \"трансформер\", которая получила широкое распространение в задачах NLP, а в последние годы и в компьютерном зрении.\n",
    "\n",
    "Существует два основных варианта трансформера: энкодер+декодер, либо только энкодер. Мы разберем оба варианта. Трансформер - это довольно замысловатый механизм, который не получится понять \"с наскока\". Его нужно изучать постепенно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde93b90-d44e-4243-a5c5-b9ad911e2cb2",
   "metadata": {},
   "source": [
    "## Общее устройство транформера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da338fc0-d7d6-47ec-84d8-1b4f4b0a871e",
   "metadata": {},
   "source": [
    "### Задача, решаемая трансформером\n",
    "\n",
    "Трансформер, состоящий из энкодера и декодера, решает задачу преобразования последовательности в последовательность (**sequence to sequence**, seq2seq). Последовательности могут быть разнообразными, но чаще всего трансформер применяется к задаче обработки текста. Типичная задача - машинный перевод текста с одного языка на другой.\n",
    "\n",
    "Сначала текст разбивается на токены (это могут быть слова целиком, части слов или отдельные буквы, в зависимости от конкретной реализации). Затем токены преобразуются в вектора-эмбеддинги. Подробнее о том, что такое вектора-эмбеддинги и откуда они берутся, можно почитать в [курсе NLP от Лены Войты](https://lena-voita.github.io/nlp_course/word_embeddings.html) (там же, кстати, можно найти и про [трансформеры](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro)). Текст, преобразованный в эмбеддинги, является массивом данных с тремя осями. Как обычно, Первая ось отвечает за номер примера в батче. Вторая ось отвечает за номер токена, а третья ось за позицию в эмбеддинге (индекс внутри вектора-эмбеддинга). Это общий принцип работы с текстом, который применяется не только в трансформерах, но и в рекуррентных сетях (RNN).\n",
    "\n",
    "Таким образом, **трансформер работает с последовательностями векторов**, то есть с массивами чисел с тремя осями: номер примера в батче, номер вектора в последовательности и позиция внутри вектора. Ниже приведена схема архитектуры трансформер. Между блоками по стрелкам передаются последовательности векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338378a-5653-4f49-bf51-dca2947affc0",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b1917-33e3-47e7-876f-325a92c2500f",
   "metadata": {},
   "source": [
    "Начнем поэтапно изучать эту архитектуру. Трансформер состоит из **энкодера** (синий) и **декодера** (оранжевый). Обе эти части состоят из последовательности блоков, притом в энкодере эти блоки устроены несколько проще, а в декодере несколько сложнее.\n",
    "\n",
    "### Блоки трансформера\n",
    "\n",
    "Пока что мы будем рассматривать блок трансформера как \"черный ящик\", не заглядывая внутрь. \"Под капотом\" этого блока - скалярное произведение, взвешенное среднее, матричное умножение и другие преобразования, но это мы расмотрим позже. Сейчас важно понять какими в целом свойствами обладает этот блок. Он принимает и возвращает последовательность векторов равной длины. Блок трансформера обладает свойством **эквивариантности к перестановкам**. Это означает, что если осуществить перестановку элементов входной последовательности, то элементы выходной последовательности окажутся переставлены таким же образом, и больше никак не изменятся. Иными словами, порядок следования элементов влияет лишь на порядок следования результатов и больше ни на что. Это означает, что блок трансформера *работает с последовательность входных векторов как с неупорядоченным множеством*.\n",
    "\n",
    "Можно провести аналогию: в школьный класс попадают ребята с разным характером и в процессе общения оказывают друг на друга так, что характеры меняются. В итоге каждый из ребят становится слегка не тем, кем был, а может и полностью измениться. Ребят в классе можно как-то упорядочить, например по имени, но по существу они являются неупорядоченным множеством. Так же и внутри блока трансформера вектора как бы \"обмениваются информацией\" и при этом изменяются.\n",
    "\n",
    "Блок трансформера может обрабатывать последовательность векторов произвольной длины. Это важно для машинного перевода, где длина переводимого текста может быть разной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ca32a-b7b1-4be2-970e-a27ce6187ae3",
   "metadata": {},
   "source": [
    "### Энкодер трансформера\n",
    "\n",
    "Энкодер состоит из последовательно соединенных блоков трансформера. В оригинальной статье предлагается использовать 6 блоков, но в более современных трансформерах их число доходит до 96. Энкодер принимает на вход вектора-эмбеддинги входных токенов, то есть векторные представления тех слов, которые надо перевести (на positional encoding пока не смотрим, о нем далее). Возвращает энкодер также набор векторов, который называется **sentence representation**. Это массив данных, в котором закодирована информация о входной последовательности, то есть смысл переводимого текста. Эта информация затем передается в декодер.\n",
    "\n",
    "До появления трансформеров в задачах seq2seq использовались в основном рекуррентные сети (RNN). Они также состоят из энкодера и декодера, но их слабое место в том, что из энкодера в декодер передавался вектор фиксированной длины. Если текст, который нужно перевести, очень длинный, то сеть может не справиться с задачей уместить всю информацию о тексте в этот вектор, в итоге качество перевода ухудшается с увеличением размера входного текста (хотя ничего не мешает разбить текст на предложения и переводить их отдельно). Трансформеры в этом плане являются шагом вперед. В них sentence representation, как видно из схемы, является последовательностью векторов такой же длины, что и входная последовательность. То есть чем длиннее входные данные, тем больший размер имеет массив sentence representation и тем больше информации может уместить.\n",
    "\n",
    "Важно отметить, что в энкодере (а также и в декодере) веса всех блоков разные, то есть это не один и тот же блок, применяемый несколько раз (как в RNN), а разные блоки с разной инициализацией весов.\n",
    "\n",
    "### Позиционное кодирование\n",
    "\n",
    "Если каждый блок трансформера эквивариантен к перестановкам, то и весь энкодер в целом также эквивариантен к перестановкам. Однако в тексте порядок слов может влиять на смысл текста. Если перестановка слов приведет лишь к перестановке соответствующих выходов декодера, то это говорит о том, что вряд ли энкодер извлекает смысл текста.\n",
    "\n",
    "Это серьезная проблема, и авторы находят ей решение. К массиву векторов, подаваемому на вход энкодеру, прибавляются слагаемые по определенному правилу. Авторы предлагают два варианта:\n",
    "\n",
    "*Вариант 1*. Каждому *номеру* элемента последовательности ставится в соответствие обучаемый вектор, называемый позиционным эмбеддингом. Если, например, эмбеддинги слов имели длину 300, а максимально допустимая длина входной последовательности - 1024, то создается массив чисел размером 1024x300 и инициализируется случайными значениями. Когда на вход подается последовательность векторов, то *к i-му вектору прибавляется эмбеддинг i-й позиции*. Позиционные эмбеддинги обучаются вместе со всей остальной нейросетью.\n",
    "\n",
    "У этого варианта есть недостаток: длина входной последовательности становится ограниченной. Слишком длинную последовательность трансформер обработать не сможет, так как не хватит позиционных эмбеддингов. Есть и второй недостаток: если максимальную длину мы зададим равной 1024, то входные последовательности длиной 1000+ будут встречаться редко, поэтому и веса соответствующих позиционных эмбеддингов будут обновляться редко, из-за чего они могут плохо обучиться.\n",
    "\n",
    "*Вариант 2*. Мы точно так же создаем позиционные эмбеддинги, но заполняем их синусоидами с разным периодом и делаем необучаемыми (замораживаем веса):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7207ad-3a71-4cbf-b11e-4930dc6fdaee",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer7.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274db6f-f339-4aab-a74b-a39525841822",
   "metadata": {},
   "source": [
    "Изображение взято из [данного источника](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers), там же можно почитать подробнее про позиционное кодирование. Кажущийся больший период синусоиды с левой стороны изображения обусловлен стробоскопическим эффектом.\n",
    "\n",
    "Авторы сравнили эти два подхода экспериментально и они дали приблизительно равную точность сети на валидации, то есть можно выбирать любой из них. Авторы предлагают выбирать вариант с сигусоидами, поскольку в нем нет ограничения на длину последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7103ea-3169-4fc7-85cb-1de81b17c814",
   "metadata": {},
   "source": [
    "### Принцип обучения трансформера\n",
    "\n",
    "Понять устройство декодера (masked transformer block) будет проще, если мы сначала рассмотрим как обучается трансформер. Обучается он, как обычно, градиентным спуском. Для простоты возьмем размер батча равный единице. Пусть на очередном шаге градиентного спуска мы имеем пару из исходных данных (предложение на английском) и целевых данных (перевод на русский). Исходные данные переводятся в эмбеддинги, складываются с позиционными эмбеддингами и пропускаются через энкодер. Мы получаем последовательность выходных векторов (sentence representation), которая должна кодировать смысл текста.\n",
    "\n",
    "Декодер при этом обучается осуществлять следующее преобразование:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18b70d-2fa0-4c87-af1f-68a19db774a9",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer2.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629de49-fe9a-46e2-9aff-c6a8ae4bfa02",
   "metadata": {},
   "source": [
    "Декодер также является последовательностью блоков трансформера (с некоторыми мофикациями, о которых далее), поэтому длины входной и выходной последовательностей в декодере должны сопадать, что и видно на схеме. START и END - это специальные токены, которые тоже имеют свои эмбеддинги. Несмотря на то, что в схеме много стрелок, вся последовательность преобразуется декодером за один проход. Стрелки означают лишь соответствие между словами.\n",
    "\n",
    "Здесь важно отметить, что речь идет об *обучении*, поэтому у нас в распоряжении есть верный ответ, который мы подаем на вход декодеру. Но тогда кажется, что задача декодера тривиальна. Зачем ему вообще нужна информация из энкодера, если верный ответ уже передан на вход, и нужно просто передать его на выход, сдвинув слова влево? И действительно, в таком виде архитектура вряд ли сможет обучиться переводить текст. Для обучения нам нужно маскирование.\n",
    "\n",
    "### Маскирование в декодере\n",
    "\n",
    "Декодер, как и энкодер, состоит из последовательности блоков. Вспомним, что в обычном трансформер-блоке (без маскирования) все входные вектора влияют на все выходные вектора. **При использовании маскирования i-й входной вектор перестает оказывать влияние на все выходные вектора от 0-го до (i-1)-го.** Это означает, что на схеме информация не может распространяться влево (от конца к началу текста), и для любого i первые i выходных векторов однозначно определяются первыми i входными векторами. При этом, конечно, теряется свойство эквивариантности к перестановкам. \n",
    "\n",
    "Например, слово \"что\" стоит на 4-й позиции. При проходе через первый блок декодера (которых всего 6) первые 3 выходных вектора не будут никак зависеть от слова \"что\". Например, если мы изменим эмбеддинг этого слова, то первые три вектора никак не изменятся. Весь декодер - это 6 последовательных блоков, поэтому для всего декодера в целом верен тот же принцип. А значит для предсказания слова \"все\" на выходе декодер не сможет использовать входное слово \"все\", а сможет использовать только слова \"<START> Я смею\" и информацию из энкодера, извлеченную из английского текста. Задача обучения для декодера перестает быть тривиальной.\n",
    "    \n",
    "### Использование информации из энкодера\n",
    "    \n",
    "Блок трансформера в энкодере принимал на вход последовательность векторов и возвращал последовательность такой же длины. Если декодер был бы устроен так же, то как он смог бы использовать еще и информацию из энкодера? Никак, поэтому блок декодера, помимо маскирования, имеет еще одно отличие от блока энкодера. Блок декодера принимает на вход сразу две последовательности векторов: одну с \"нижнего уровня\" декодера и другую из энкодера. Эти последовательности могут иметь разную длину. Возвращает же декодер последовательность длиной равной той, что взята с \"нижнего уровня\".\n",
    "    \n",
    "### Инференс\n",
    "    \n",
    "Если при обучении мы сразу имеем эталонный ответ, который можем передать на вход декодеру, то как мы будем поступать при инференсе, то есть когда нам нужно перевести неивестную фразу? Мы делаем таким образом. Сначала мы обрабатываем фразу энкодером и получаем для нее внутренние представления. Затем на вход декодеру мы подаем только слово START. На выходе мы получаем первое слово перевода, допустим \"Я\". Затем на вход мы подаем слова [START, Я] и получаем выход [Я, смею]. Затем подаем на вход слова [START, Я, смею] и так далее, пока не получим слово END.\n",
    "    \n",
    "Получается, что во время инференса декодер работает в рекуррентном режиме, а при обучении нерекурретно (параллельно). Возможно именно поэтому он назван \"трансформером\" - потому что трансформируется из паралелльного режима в рекуррентный. Обучение в параллельном режиме дает огромный прирост производительности при обучении, по сравнению с RNN.\n",
    "    \n",
    "Зато складывается ощущение, что во время инференса трансформер работает очень мелленно - ведь он запускается столько раз, сколько слов в переводе. Однако это не совсем так. Вспомним про маскирование: информация не передается в декодере от конца к началу. Поэтому при каждом следующем слове нам не нужно пересчитывать все, что посчитал декодер ранее - ведь информация назад не распространяется! Поэтому общий объем необходимых вычислений остается таким же, как при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b9351-795a-4360-b978-eb38c66f78d0",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer3.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1c2d4-e4a7-4247-9bdb-051c115aa246",
   "metadata": {},
   "source": [
    "## Устройство блока трансформера\n",
    "\n",
    "Как мы ранее обсуждали, блок энкодера принимает и возвращает последовательность векторов произвольной длины и имеет свойство перестановочной эквивариантности (порядок следования элементов влияет лишь на порядок следования результатов). Далее поэтапно разберем внутреннее устройство такого блока, начав с механизма внимания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977a0a9-a5a3-42cb-8a5b-39b16d11e4d2",
   "metadata": {},
   "source": [
    "### Dot-product attention\n",
    "\n",
    "Механизм \"внимания\" (attention) был впервые описан в статье [Neural Machine Translation by Jointly Learning to Align and Translate]($Neural Machine Translation by Jointly Learning to Align and Translate$) (D. Bahdanau, K. Cho, Y. Bengio, 2014), и с тех пор было предложено много его вариантов. Рассмотрим этот механизм сначала в общем виде.\n",
    "\n",
    "Пусть мы имеем вектор $Q$ и множество векторов $\\{V_i\\}$. Размер множества $\\{V_i\\}$ может быть переменным, и известно, что некоторые векторы из $\\{V_i\\}$ имеют большее отношение к $Q$, чем другие. Например, $Q$ может быть эмбеддингов одного из слов в тексте, а $\\{V_i\\}$ - эмбеддинги всех остальных слов. Мы выполняем с этими данными следующее преобразование:\n",
    "\n",
    "1. Для каждого вектора $V_i$ считаем число $S_i = score(Q, V_i)$, описывающее степень его релевантности $Q$.\n",
    "2. К полученным числам применяем операцию softmax, получая веса: $W_0, W_1, \\ldots, W_n = softmax(S_1, S_2, \\ldots, S_n)$.\n",
    "3. Считаем взвешенное среднее элементов $\\{V_i\\}$, используя веса $\\{W_i\\}$: $V = W_0V_0 + W_1V_1 + \\ldots + W_nV_n$.\n",
    "\n",
    "Вектор $V$ и является нашим результатом. По задумке он аккумулирует в себе информацию об элементах из $\\{V_i\\}$, наиболее релевантных вектору $Q$, который называется \"контекстом\", или \"запросом\" (**query**). Таким образом, механизм внимания - это преобразование, которое принимает на вход данные переменного размера и возвращает вектор фиксированного размера.\n",
    "\n",
    "Функция $score$ может быть обучаемой или фиксированной. В самом простом случае это скалярное произведение: $score(Q, V_i) = Q \\cdot V_i$. Такой вариант называется **dot product attention**, и именно он используется в трансформерах. Часто скалярное произведение неформально описывается как мера сходства двух векторов, однако это не совсем так. Более точным будет утвердение, что скалярное произведеение - это величина проекции вектора $V_i$ на направление, задаваемое вектором $Q$, умноженная на длину $Q$.\n",
    "\n",
    "В более общем случае вместо множества векторов $\\{V_i\\}$ мы можем иметь множества пар ключ-значение: $\\{K_i\\}$ и $\\{V_i\\}$. В этом случае для сравнения с запросом $Q$ мы используем ключи $\\{K_i\\}$, получаем веса и с их помощью считаем средневзвешенное значений $\\{V_i\\}$:\n",
    "\n",
    "1. Для каждого ключа $K_i$ считаем число $S_i = score(Q, K_i)$, описывающее степень его релевантности запросу $Q$.\n",
    "2. К полученным числам применяем операцию softmax, получая веса: $W_0, W_1, \\ldots, W_n = softmax(K_1, K_2, \\ldots, K_n)$.\n",
    "3. Считаем взвешенное среднее элементов $\\{V_i\\}$, используя веса $\\{W_i\\}$: $V = W_0V_0 + W_1V_1 + \\ldots + W_nV_n$.\n",
    "\n",
    "Предыдущий алгоритм является частным случаем, когда $K_i = V_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4faf08-8d79-4fe2-9fdc-8fe6472dbc4d",
   "metadata": {},
   "source": [
    "### Scaled dot-product attention\n",
    "\n",
    "С увеличением размерности векторов $Q$ и $\\{K_i\\}$ их скалярное произведение (стреднестатистически) также начинает расти. Например, если случайные величины $Q$ и $K$ являются векторами длины $L$, элементы которых имеют [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) распределение $\\mathcal{N}(0, 1)$, то их скалярное произведение имеет распределение $\\mathcal{N}(0, L)$, то есть дисперсия тем выше, чем больше длина векторов. А как известно, функция softmax чувствительна к масштабу входных данных. Если входные данные имеют сильно отличающиеся друг от друга значения, то на выходе softmax вероятно будут числа, близкие и единице и нулям, и производные результата по входам будет стремиться к нулю. Это мешает обучению (\"проблема затухающих градиентов\").\n",
    "\n",
    "Для избавления от этой проблемы авторы предлагают домножать $S_i$ на квадратный корень из размерности вектора $Q$:\n",
    "\n",
    "$L = dim(Q)$\n",
    "\n",
    "$S_i = \\cfrac{score(Q, K_i)}{\\sqrt{L}}$\n",
    "\n",
    "Такая модификация называется scaled dot-product attention и используется в трансформерах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da3e0f-8bbb-440e-a59e-24592e25d3d7",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer4.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec766392-34f6-45f8-8dc4-bbb8eb6077b2",
   "metadata": {},
   "source": [
    "$Attention(Q, K, V) = softmax \\bigg( \\cfrac{QK^T}{\\sqrt{dim(K)}} \\bigg) V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ac46e-c95b-4d0f-ad5f-fab3ff28d227",
   "metadata": {},
   "source": [
    "**Резюме:** на основании степени \"сходства\" (по скалярному произведению) ключей $\\{K_i\\}$ с запросом $Q$ рассчитываются веса $\\{W_i\\}$, которые означают релевантность векторов $\\{V_i\\}$ запросу. Затем считается взвешенное среднее векторов $\\{V_i\\}$ с использованим этих весов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f281e8d-136d-4d23-af3d-29ca20e88f1e",
   "metadata": {},
   "source": [
    "### Multi-query dot-product attention\n",
    "\n",
    "Для одного и того же множества $\\{K_i\\}$, $\\{V_i\\}$ мы можем по очереди использовать несколько векторов $Q$ и получить несколько результатов. Обозначим описанное на схеме выше преобразование как `single_query_attention(Q, K, V)`. Тогда преобразование для несколько векторов $Q$ можно записать таким псевдокодом:\n",
    "\n",
    "```\n",
    "function multi_query_attention(Queries, Keys, Values):\n",
    "    for i in [0, ..., L-1]:\n",
    "        Outputs[i] = single_query_attention(Queries[i], Keys, Values)\n",
    "    Возвращаем массив Outputs\n",
    " ```\n",
    " \n",
    "То есть мы применяем механизм внимания N раз, по очереди используя каждый из векторов как контекст. Ниже приводится пример того, как на Python можно запрограммировать Scaled Dot Product Attention. Код вычислительно не оптимизирован, приводится лишь для примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6948bb5-4f5a-417b-8330-bb65730cb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def single_query_attention(query, keys, values):\n",
    "    S = np.array([(k*query).sum() for k in keys])  # scalar product\n",
    "    S /= np.sqrt(len(query)) # scaling\n",
    "    W = softmax(S)\n",
    "    result = np.average(values, axis=0, weights=W) # weighted mean\n",
    "    return result\n",
    "\n",
    "def multi_query_attention(queries, keys, values):\n",
    "    return np.array([single_query_attention(q, keys, values) for q in queries])\n",
    "\n",
    "# check\n",
    "queries = np.random.rand(20, 256)\n",
    "keys = np.random.rand(50, 256)\n",
    "values = np.random.rand(50, 512)\n",
    "assert multi_query_attention(queries, keys, values).shape == (20, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cee089-e571-4a41-b38f-ba82a4173b25",
   "metadata": {},
   "source": [
    "В качестве частного случая мы можем передать одну и ту же последовательность и в качестве $Q$, и в качестве $K$, и в качестве $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e5741-2736-43db-a717-3fd688d1ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = multi_query_attention(inputs, inputs, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffb7c6-a40d-4334-a834-6eae935a7a5d",
   "metadata": {},
   "source": [
    "Таким образом, multi-query dot-product attention принимает на вход три последовательности векторов:\n",
    "\n",
    "- **Queries** длиной $L_q$ и размерностью $D_q$\n",
    "- **Keys** длиной $L_v$ и размерностью $D_q$\n",
    "- **Values** длиной $L_v$ и размерностью $D_v$\n",
    "\n",
    "И возвращает последовательность векторов длиной $L_q$ и размерностью $D_v$. При этом векторы либо \"обмениваются информацией\" друг с другом (если $Q = V$), либо обновляются под действием информации извне (если $Q \\neq V$).\n",
    "\n",
    "*Примечание. Понятие \"multi-query attention\" не общепринято и используется лишь в данном обзоре для понятности.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ff946-a11a-4d64-9aac-9a8237f3a66e",
   "metadata": {},
   "source": [
    "### Masked attention\n",
    "\n",
    "Допустим мы передаем в Scaled Dot Product Attention одну и ту же последовательность и в качестве $Q$, и в качестве $K$, и в качестве $V$. Маскирование заключается в том, что на i-м шаге (то есть работая с $Q_i$) мы используем только первые $i$ векторов из $K$ и $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf33becc-e3c3-4d6f-8d76-e076fd5d77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_attention(queries, keys, values, is_masked=False):\n",
    "    if is_masked:\n",
    "        assert np.array_equal(queries, values)\n",
    "        assert np.array_equal(keys, values)\n",
    "        return np.array([\n",
    "            single_query_attention(q, keys[:i+1], values[:i+1])\n",
    "            for i, q in enumerate(queries)\n",
    "        ])\n",
    "    else:\n",
    "        return np.array([\n",
    "            single_query_attention(q, keys, values)\n",
    "            for q in queries\n",
    "        ])\n",
    "\n",
    "# check\n",
    "inputs = np.random.rand(20, 512)\n",
    "assert multi_query_attention(inputs, inputs, inputs, is_masked=True).shape == inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f62f0-fcb9-46fe-bb25-62f2dd3aa8ed",
   "metadata": {},
   "source": [
    "Вместо того, чтобы использоватьв Masked Attention только первые $i$ векторов из $K$ и $V$, мы можем использовать все вектора, но часть посчитанных скалярных произведений заменять на минус бесконечность. После применения softmax соответствующие им веса будут нулевыми. Два варианта, показанные на схеме ниже, идентичны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ed48b-3a24-48f4-b587-1ac8d0620336",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer9.jpg\" width=\"700\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f40fbb-db9f-4a9a-b4a5-01b1c65e2963",
   "metadata": {},
   "source": [
    "Вычислительно эффективная реализация Dot Product Attention использует матричное умножение, при котором считаются попарные скалярные произведения между всеми векторами из $Q$ и $K$. После этого на результат накладывается маска, согласно которой часть скалярных произведений заменяется на минус бесконечность. После применения softmax минус бесконечность становится нулем.\n",
    "\n",
    "Таким образом, **маскирование \"запрещает\" некоторые связи между входными векторами**. В декодере трансформера маскирование используется для того, чтобы последующие входные вектора не оказывали влияния на предыдущие выходные (auto-regressive property). Но маскирование может иметь и более широкое применение. Например, с его помощью можно ограничить связи между отдаленными элементами последовательности (как на второй иллюстрации ниже). Эта тема будет еще раз затронута ниже в разделе \"Обсуждение архитектуры\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3f7a8-214b-48ea-a876-91edba6f2769",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer10.jpg\" width=\"700\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9351452-92be-4342-9d58-0df0681a7de6",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "Авторы трансформера предлагают еще одну модификацию этой операции: multi-head attention. Вместо одного блока Attention предлагается использовать несколько параллельно работающих блоков, результаты которых конкатенируются. Сам по себе блок multi-query attention не имеет обучаемых параметров, и использование нескольких параллельно работающих блоков, принимающих одни и те же данные, бессмысленно: все блоки будут работать идентично. Поэтому для каждого блока (\"головы\") и каждого входа создается обучаемое линейное преобразование."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18745a2c-5486-45e3-be3d-53f4099a2caa",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer5.jpg\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e757b-8e78-4d03-84c9-8ad8b12c8d64",
   "metadata": {},
   "source": [
    "Слой *\"Linear (axis=-1)\"* на схеме означает следующее: мы создаем линейный (полносвязный) слой без функции активации, преобразующий $N_1$ чисел в $N_2$ чисел. Этот слой обрабатывает каждый вектор в последовательности независимо.\n",
    "\n",
    "Конкатенация *\"Concat (axis=-1)\"* соединяет вектора по оси размерности, то есть, например, первый вектор результирующей последовательности равен склейке первого вектора из каждой головы.\n",
    "\n",
    "Авторы предлагают использовать 8 голов, а значит нужно создать 24 обучаемых линейных преобразования, плюс еще одно, выполняемое после конкатенации. Смысл линейных преобразований не только в том, что они позволяют блокам работать по-разному, но и в том, что они сокращают размерность векторов. Авторы предлагают использовать $N_1$=512 и $N_2$=64. После конкатенации мы снова получаем вектора размерностью 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321cd48-b3e2-4a42-982e-91683da2460d",
   "metadata": {},
   "source": [
    "### Блоки трансформера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38e7dc-290c-4f88-a93c-853090cab24f",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer6.jpg\" width=\"1000\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a74654-85b8-4df4-a0bb-6a0d6164a95c",
   "metadata": {},
   "source": [
    "В **блоке энкодера** мы применяем Multi-Head Attention, подставляя входную последовательность и в качестве $Q$, и в качестве $K$, и в качестве $V$. Таким образом, каждый вектор входной последовательности по очереди \"смотрит\" на другие вектора и обновляется под их воздействием. Вокруг Multi-Head Attention проброшена связь (residual connection):\n",
    "\n",
    "`outputs = inputs + Dropout(MultiHeadAttention(inputs))`\n",
    "\n",
    "Skip connections стали активно использовать после появления сети [ResNet]($Deep Residual Learning for Image Recognition$), где была продемонстрирована их эффективность. Наличие skip connection, проброшенного вокруг некоего блока, означает, что этот блок работает \"аддитивно\", то есть добавляет результаты своих вычислений к массиву данных и тем самым \"корректирует\" его, вместо того, чтобы полностью изменять массив данных. Сам блок при этом называется resudial block (остаточный блок). Есть [работа]($Residual Networks Behave Like Ensembles of Relatively Shallow Networks$), в которой показывается, что сеть со skip connections эквивалентна ансамблю менее глубоких сетей.\n",
    "\n",
    "Слой [LayerNormalization]($Layer Normalization$) нормализует каждый вектор по отдельности. Это можно записать на python следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b41e47-0bb3-4817-82ce-9ae690f1b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_normalization(inputs):\n",
    "    means = inputs.mean(axis=-1, keepdims=True)\n",
    "    stds = inputs.std(axis=-1, keepdims=True)\n",
    "    return (inputs - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536459e-be8c-45f6-9336-de25be517550",
   "metadata": {},
   "source": [
    "Также опционально Layer Normalization может содержать обучаемый масштаб и смещение, как описано [здесь](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
    "\n",
    "После LayerNormalization в блоке энкодера находится Feed forward network: два линейных слоя с ReLU между ними, которые тоже обрабатывают каждый вектор независимо. Это можно представить также как свертку 1х1. Вектор длиной 512 превращается в вектор длиной 2048 после первого слоя, и снова в вектор длиной 512 после второго слоя. Вокруг Feed forward network тоже проброшен residual connection. Далее снова идет слой LayerNormalization.\n",
    "\n",
    "В более новой [работе]($On Layer Normalization in the Transformer Architecture$) предлагается помещать слои LayerNormalization внутрь resudial блоков, то есть после линейного слоя и до суммирования, что делает обучение более стабильным.\n",
    "\n",
    "В каждом residual-блоке перед суммированием применяется dropout с rate=0.1. Также dropout применяется перед первым блоком в энкодере и декодере.\n",
    "\n",
    "**Блок декодера** отличается от блока энкодера двумя особенностями. Во-первых, в нем применяется Multi-Head Attention с маскированием. Во-вторых следом применяется еще один Multi-Head Attention, в котором в качестве keys и values используется последовательность векторов, полученная из энкодера. Получается, что сначала вектора \"смотрят\" друг на друга и обмениваются информацией, а затем \"смотрят\" на выход энкодера и получают из него информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a039671-2369-4aeb-9a1b-9f3acfa052cb",
   "metadata": {},
   "source": [
    "### Выходной слой декодера\n",
    "\n",
    "Последний блок декодера возвращает последовательность векторов. Каждый вектор нужно каким-то образом преобразовать в слово. Вспомним, что на входе сети у нас есть матрица эмбеддингов, которая сопоставляет вектор каждому слову из словаря. Если какой-либо из выходных векторов мы умножим на эту матрицу, а затем применим argmax (как в задаче классификации), то мы найдем слово, эмбеддинг которого имеет с данным вектором наибольшее скалярное произведение. Это слово мы и будем считать выходным.\n",
    "\n",
    "Таким образом, каждый вектор, полученный на выходе декодера, умножается на матрицу эмбеддингов. Это означает добавление нового линейного слоя, который имеет общие веса (shared weights) с матрицей эмбеддингов. В результате мы получаем вектора, длина которых равна размеру словаря. Применив softmax, мы получим вероятности слов. Применим argmax, получим наиболее вероятное слово.\n",
    "\n",
    "На этом рассмотрение архитектуры трансформера можно считать завершенным."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb192d-994a-454e-b1df-b95fe7ede619",
   "metadata": {},
   "source": [
    "### Обучение\n",
    "\n",
    "Авторы обучали трансформер на задаче машинного перевода (English-German, English-French), объединяя в батчи примеры с примерно равной длиной. Использовался оптимизатор Adam, рост learning rate в течение первых 4000 шагов, затем полиномиальное затухание со степенью 0.5 (деление learning rate на квадратный корень из номера шага) в течение 100 тысяч шагов для малой модели и 300 тысяч шагов для большой модели. Большая модель отличалась от малой увеличенным с 512 до 1024 размерностью векторов и 16 головами вместо 8 (более детально см. таблицу 4 в статье). Для регуляризаци, помимо dropout, использовался label smoothing. Обучение большой модели заняло 3.5 дня на 8 видеокартах Nvidia P100.\n",
    "\n",
    "Для инференса на тестовом датасете использовалось усреднение по последним 20 чекпоинтам (то есть 20 сохраненным состояниям модели в процессе обучения) и алгоритм [beam search](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#seq2seq_inference).\n",
    "\n",
    "В результате побит рекорд метрики качества и установлен новый state-of-the-art на задачах машинного перевода.\n",
    "\n",
    "Авторы также тестируют трансформер на задаче [синтаксического разбора](http://nlpprogress.com/english/constituency_parsing.html) и достигают хороших результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf2061-8828-4de9-987a-15b81f5f60a9",
   "metadata": {},
   "source": [
    "### Обсуждение архитектуры\n",
    "\n",
    "Авторы сравнивают механизмом внимания (self-attention) с альтернативными подходами к обработке последовательностей. В таблице сравниваются четыре похода:\n",
    "1. Self-attention, используемый в трансформере\n",
    "2. Рекуррентный слой\n",
    "3. Сеть из нескольких сверточных слоев с ядром размера $k$ и subsampling'ом между ними (pooling или dilated convolutions)\n",
    "4. Вариант self-attention, в котором \"внимание\" каждого вектора направлено лишь на $r$ ближайших к нему векторов.\n",
    "\n",
    "Поскольку параметры (количество слоев и прочее) строго не заданы, исользуется $O$-нотация. За $n$ обозначена длина последовательности, на $d$ размерность векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f9fde-17fa-4f61-8c85-ae3cbd96dc0d",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2819c3-48f8-47cc-a6c9-64d42f9e3364",
   "metadata": {},
   "source": [
    "**Первый столбец** означает количество требуемых арифметических операций. Если $n < d$, что является наиболее частым случаем, то self-attention требует меньшего количества операций, чем рекуррентная сеть. Однако квадратичная сложность операции attention в зависимости от длины последовательности $O(n^2 * d)$ становится проблемой при обработке длинных последовательностей. Например, если на входе тысяча векторов, то нужно посчитать миллион скалярных произведений. Авторы высказывают гипотезу о том, что self-attention можно было бы ограничить так, чтобы каждый вектор обменивался информацией только с $r$ ближайшими соседями (последняя строка таблицы). В более новых работах предложены [разреженные трансформеры]($Generating Long Sequences with Sparse Transformers$), в которых вычислительная стоимость операции self-attention уменьшена с $O(n^2)$ до $O(n \\sqrt n)$.\n",
    "\n",
    "Сверточные слои имеют более высокую сложность вычислений $O(k*n*d^2)$, однако с помощью separable convolutions можно уменьшить сложность до $O(k*n*d + n*d^2)$\n",
    "\n",
    "**Второй столбец** описывает возможность параллелизации, то есть максимальную длину пути между входами и выходами в графе вычислений. Self-attention, как и свертки, хорошо параллелизуется, в отличие от рекуррентных сетей.\n",
    "\n",
    "**Третий столбец** описывает длину пути в графе вычислений, по которой можно пройти от первого до последнего элемента последовательности (если граф вычислений представить как неориентированный). Авторы называют это \"path length between long-range dependencies\". Известно, что чем меньше длина такого пути, тем эффективнее сеть обучается, то есть тем легче обнаруживает взаимосвязи между отдаленными элементами в последовательности.\n",
    "\n",
    "> Learning long-range\n",
    "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
    "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
    "traverse in the network. The shorter these paths between any combination of positions in the input\n",
    "and output sequences, the easier it is to learn long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7c132-57cf-4b45-b86d-db9e1ef4508f",
   "metadata": {},
   "source": [
    "Наконец, авторы высказывают мысль о том, что трансформеры могут оказаться более интерпретируемыми моделями, чем RNN и сверточные сети. \"Внимание\", направленное от одних элементов последовательности к другим (то есть скалярные произведения этих элементов) легко визуализируются, и выясняется, что отдельные \"головы\" multi-head attention обучаются искать взаимосвязи разных типов, что авторы показывают на иллюстрациях:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08a4e5-950a-4be4-928e-cbf7d1fd328a",
   "metadata": {},
   "source": [
    "<img src=\"assets/transformer8.jpg\" width=\"600\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d02d93-b898-4b1e-ae4e-8a8fe910d0cc",
   "metadata": {},
   "source": [
    "В последующих работах был проведен более детальный анализ, об этом можно почитать [здесь](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#analysis_interpretability) и [здесь]($Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned$).\n",
    "\n",
    "В ходе экспериментов было обнаружено, что уменьшение размера ключа в self-attention сильно ухудшает производительность, поэтому авторы высказывают гипотезу о том, что поиск векторов, наиболее релевантных данному, не является легкой задачей для сети, поэтому стоит задуматься об использовании более сложной функции, чем скалярное произведение.\n",
    "\n",
    "В [одной из последующих работ]($Pretrained Transformers as Universal Computation Engines$) было показано, что трансформеры могут претендовать на роль универсальной архитектуры нейронных сетей: обученный на NLP-задаче трансформер с минимальным файн-тюнингом может использоваться и на совсем других задачах, таких как компьютерное зрение и предсказание структуры белка."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7391323-8c2c-403d-915d-ebe458308bcf",
   "metadata": {},
   "source": [
    "### Трансформер без декодера\n",
    "\n",
    "В оригинальной статье не упоминается трансформер без декодера, однако стоит о нем также кратко упомянуть. В статье [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]($BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding$) трансформер без декодера обучается либо подставлять пропущенные слова в тексте, либо предсказывать, является ли один кусок текста продолжением другого. Такой подход позволяет обучать трансформер на неразмеченных данных. Полученную модель можно файн-тюнить под широкий класс задач.\n",
    "\n",
    "Для задачи классификации подаваемое в энкодер предложение начинается со специального токена [CLS]. К выходному вектор энкодера, полученный на данном токене, затем присоединяется полносвязный слой, решающий задачу классификации. Подробнее об этом можно почитать, например, [здесь](https://habr.com/ru/post/487358/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
