{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd10562e-c7f4-4b41-ab9f-8373ff26f9f9",
   "metadata": {},
   "source": [
    "Данная статья посвящена алгоритму AdaBoost и написана авторами этого алгоритма. Но впервые этот алгоритм предложен в [другой статье]($A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting$), более длинной и наполненной математическими выкладками, теоремами и доказательствами. Соответственно, данная статья больше \"для людей\". В ней рассматривается только вариант алгоритма AdaBoost для задачи бинарной классификации, в которой weak learner'ы выдают -1 или 1.\n",
    "\n",
    "Принцип работы алгоритма AdaBoost я описывал в [этой статье]($Алгоритм AdaBoost$).\n",
    "\n",
    "В текущей статье (со ссылкой на другую статью) утверждается, что сходимость AdaBoost доказана: если существует некое число $\\epsilon < 0.5$ такое, что взвешенная ошибка $\\epsilon_t$ каждого weak learner'а меньше $\\epsilon$, то ошибка на обучающем датасете экспоненциально стремится к нулю.\n",
    "\n",
    "Далее идет теоретическая оценка обобщающей способности алгоритма (опять со ссылкой на другую статью). По-видимому такие оценки делаются в рамках \"statistical learning\", то есть задачи, в которой обучающий и тестовый датасеты являются [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) выборками из некоего фиксированного распределения (домена). При этом доказывается, что при объеме обучающего датасета, стремящемся к бесконечности, ошибка на тесте стремится к нулю.\n",
    "\n",
    "Для задачи бинарной классификации с ответами -1 и +1 авторы предлагают ввести величину $margin(y, \\hat{y}) = y * \\hat{y}$, где $y$ - эталонный ответ, а $\\hat{y}$ - предсказание, которое является действительным числом от -1 до 1. Эта величина равна 1 если предсказание совпадает с ответом, и равна -1 если предсказание максимально далеко от ответа. Утвеждается, что AdaBoost \"концентрируется\" на примерах с наименьшим значением $margin(y, \\hat{y})$, поэтому в итоге margin'ы на всех обучающих примерах стремятся к единице. За счет этого алгоритм продолжает учиться (и ошибка на тесте при этом может продолжать убывать) даже тогда, когда точность (accuracy) на обучающем датасете достигла единицы, поскольку margin'ы еще не все равны единице.\n",
    "\n",
    "Далее в статье авторы сравнивают AdaBoost с SVM, однако это сравнение полностью теоретическое и основано на том, что SVM также максимизирует margin (ищет разделяющую гиперплоскость с наибольшим зазором). Всколзь упоминается также связь AdaBoost с линейным программированием, и теорией игр со ссылкой на статью \"Game theory, on-line prediction and boosting\".\n",
    "\n",
    "В конце статьи упоминается важная особенность AdaBoost: способность этого алгоритма находить выбросы в данных (outliers), то есть либо ошибочно классифицированные примеры, либо примеры из другого распределения, попавшие в датасет по ошибке. Выбросами можно считать примеры с наибольшими весами после нескольких раундов бустинга. Упоминаются также алгоритмы \"Gentle AdaBoost\" и \"BrownBoost\", которые уделяет меньше внимания выбросам, чем обычный AdaBoost, и потому более подходят в ситуации, когда выбросов очень много."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
